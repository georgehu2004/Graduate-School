---
title: "STAT 551 Linear Regression Assignment"
author: "Yuchi Hu"
date: "February 3, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=F,warning=F,echo=T,fig.height=4,fig.width=6,cache=F,        
                      out.extra='',fig.pos= 'h')
```

```{r, eval=F, echo=F}
# Install packages used in this homework
install.packages(ggplot2)
install.packages(gridExtra)
install.packages(knitr)
```

```{r, echo=F}
# Load packages used in this homework
library(ggplot2)
library(gridExtra)
library(knitr)
```

# Part 1
## Part 1.1

We make a scatterplot of *Y* vs. *numvar1*.

```{r, fig.cap='\\label{part1}Y vs. numvar1'}
# Load LinRegData.csv
mydata <- read.csv('C:/Users/George/Desktop/PredictiveAnalyticsI/LinRegData.csv', header=T)

theme_update(plot.title=element_text(hjust=0.5))
# Y vs. numvar1
ggplot(data=mydata, aes(y=Y, x=numvar1)) + geom_point() + labs(title='Y vs. numvar1')
```

From **Figure \ref{part1}**, the variables seem to be associated. The association appears to be linear.

The model $Y=\beta_0+\beta_1*numvar1+\epsilon$ is a simple linear regression model. It is appropriate in this case since we have only one predictor (*numvar1*), which appears to be linearly associated with the response (*Y*). 

## Part 1.2

We fit a simple linear regression model (model1) with *Y* as the response and *numvar1* as the predictor. 

```{r}
# Simple linear regression model
model1 <- lm(Y ~ numvar1, data=mydata)
```

The summary of model1 is below: 

```{r}
# Model summary
summary(model1)
```

Two parameters are estimated in model1. The parameter for the intercept estimates an average value of 5.259 units for *Y* when *numvar1* = 0. The parameter for *numvar1* estimates an average increase of 3.036 units in *Y* for each unit increase in *numvar1*.

The $R^2$ of 0.8975 means that model1 explains 89.75% of the variability in *Y*.

\newpage

# Part 2
## Part 2.1

We make a new variable *numvar1new* by raising *numvar1* to the $(1/2.25)^{th}$ power. Then, we fit a simple linear regression model (model2) with *Y* as the response and *numvar1new* as the predictor. The summary of model2 is below: 

```{r}
# Raise numvar1 to the (1/2.25)th power
mydata$numvar1new <- mydata$numvar1^(1/2.25)

# New simple linear regression model
model2 <- lm(Y ~ numvar1new, data=mydata)
# Model summary
summary(model2)
```

The parameter for the intercept estimates an average value of 1.979 units for *Y* when *numvar1new* = 0. The parameter for *numvar1new* estimates an average increase of 6.411 units in *Y* for each unit increase in *numvar1new*.

The $R^2$ of 0.9263 means that model2 explains 92.63% of the variability in *Y*. Compared to the $R^2$ of model1 (0.8975), the $R^2$ of model2 is larger, so model2 is a better fit to the data.

\newpage

# Part 3
We make three scatterplots of *Y* vs. *numvar1new*, each colored by *catvar1*, *catvar2*, or *catvar3*. 

```{r, fig.height=7, fig.width=11, fig.cap='\\label{part3}Y vs. numvar1new (Colored by catvar1, catvar2, or catvar3)'}
# Y vs. numvar1 (Colored by catvar1)
p1 <- ggplot(data=mydata, aes(y=Y, x=numvar1new, color=catvar1)) + geom_point() + 
  labs(title='Y vs. numvar1new (Colored by catvar1)')
# Y vs. numvar1 (Colored by catvar2)
p2 <- ggplot(data=mydata, aes(y=Y, x=numvar1new, color=catvar2)) + geom_point() + 
  labs(title='Y vs. numvar1new (Colored by catvar2)')
# Y vs. numvar1 (Colored by catvar3)
p3 <- ggplot(data=mydata, aes(y=Y, x=numvar1new, color=catvar3)) + geom_point() + 
  labs(title='Y vs. numvar1new (Colored by catvar3)')

# Combine the plots into one graph
grid.arrange(p1, p2, p3, ncol=2)
```

From **Figure \ref{part3}**, in the top left plot (colored by *catvar1*), only the points with "F" values for *catvar1* appear to be clearly separated from the rest of the points. In the top right plot (colored by *catvar2*), the color distinction is more defined, and in the bottom left plot (colored by *catvar3*), the color distinction is even more defined with clear separation of points with different values of *catvar3*. Thus, all of the categorical variables should be included in the model (perhaps as interaction terms with *numvar1new*) since the distribution of the levels in each factor does not appear to be random.

\newpage

# Part 4
Similar to the transformation in part 2.1, we make a new variable *numvar2new* by raising *numvar2* to the $(1/3.75)^{th}$ power. Then, we make three new models.

```{r}
# Raise numvar2 to the (1/3.75)th power
mydata$numvar2new <- mydata$numvar2^(1/3.75)

# Three new models
model3 <- lm(Y ~ numvar1new + catvar1 + catvar2 + catvar3, data=mydata)
model4 <- lm(Y ~ numvar1new*catvar1 + numvar1new*catvar2 + numvar1new*catvar3, data=mydata)
model5 <- lm(Y ~ numvar2new +numvar1new*catvar1 + numvar1new*catvar2 + numvar1new*catvar3, data=mydata)
```

We compare the adjusted $R^2$ of the five models (see table below) since not all of the models have the same number of predictors. We also compare their residual standard error (RSE).

```{r}
# Adjusted R^2 of the models
rsq1 <- summary(model1)$adj.r.squared
rsq2 <- summary(model2)$adj.r.squared
rsq3 <- summary(model3)$adj.r.squared
rsq4 <- summary(model4)$adj.r.squared
rsq5 <- summary(model5)$adj.r.squared
# RSE of the models
rse1 <- summary(model1)$sigma
rse2 <- summary(model2)$sigma
rse3 <- summary(model3)$sigma
rse4 <- summary(model4)$sigma
rse5 <- summary(model5)$sigma

# Columns for table
models <- c('model1', 'model2', 'model3', 'model4', 'model5')
rsq <- round(c(rsq1, rsq2, rsq3, rsq4, rsq5), 5)
rse <- round(c(rse1, rse2, rse3, rse4, rse5), 5)

# Table of adjusted R^2
dt <- cbind('Model'=models, 'Adjusted R-Squared'=rsq, 'RSE'=rse)
kable(dt, align='c', caption='Adjusted R-Squared and RSE Comparison')
```

From **Table 1**, we can see that model5 has the highest adjusted $R^2$ and lowest RSE; thus, model5 is the best fit to the data.

# Part 5
In each of 1000 simulations, we create a dataset of 100 random X and Y points, and we create a linear model on these points. We save the values of $\beta_1$ and create a histogram of these values.

```{r, fig.cap='\\label{part5}Histogram of $\\beta_1$'}
# Number of simulations
B <- 1000 
# Initialize vector for b1
b1 <- rep(NA, B)

for(i in 1:B){
  set.seed(i)
  X <- rnorm(100, 20, 10)  # X values
  Y <- rnorm(100, 70, 5)  # Y values
  model <- lm(Y ~ X)  # Model
  b1[i] <- model$coefficients[2] # Add slope to vector
}

# Histogram of b1
ggplot(data=NULL, aes(x=b1)) + geom_histogram() + 
  labs(title=expression(paste('Histogram of ', beta[1])), x=expression(beta[1]))
```

From **Figure \ref{part5}**, we can see that the distribution of $\beta_1$'s appears to be normal and centered at 0; thus, the estimate for $\beta_1$ is 0. This value ($\beta_1=0$) can be interpreted as the slope being 0; that is, there is not a significant association between X and Y. 
