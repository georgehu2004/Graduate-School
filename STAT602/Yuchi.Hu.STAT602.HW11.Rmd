---
title: "STAT 602 Homework 11"
author: "Yuchi Hu"
date: "April 3, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=F,warning=F,echo=F,fig.height=4,fig.width=6,cache=F,out.extra='',fig.pos='h')
```

```{r, eval=F}
# Install packages used in this homework
install.packages(ISLR)
install.packages(e1071)
install.packages(knitr)
install.packages(tree)
install.packages(randomForest)
install.packages(gbm)
install.packages(glmnet)
install.packages(boot)
install.packages(MASS)
install.packages(class)
install.packages(mclust)
```

```{r}
# Load packages used in this homework
library(ISLR)
library(e1071)
library(knitr)
library(tree)
library(randomForest)
library(gbm)
library(glmnet)
library(boot)
library(MASS)
library(class)
library(mclust)
```

# 1. Question 9.7.2 pg 368
We have seen that in p = 2 dimensions, a linear decision boundary takes the form $\beta_0 + \beta_1X_1 + \beta_2X_2 = 0$. We now investigate a non-linear decision boundary.

## (a)
Sketch the curve $$(1 + X_1)^2 + (2 - X_2)^2 = 4.$$

## Answer
I used [Desmos](https://www.desmos.com/) to sketch the curve.

```{r, out.width = "0.7\\linewidth"}
knitr::include_graphics("C:/Users/George/Desktop/HW11.Q1a.png")
```

## (b)
On your sketch, indicate the set of points for which $$(1 + X_1)^2 + (2 - X_2)^2 > 4,$$ as well as the set of points for which $$(1 + X_1)^2 + (2 - X_2)^2 \le 4.$$

## Answer
The set of points for which $(1 + X_1)^2 + (2 - X_2)^2 > 4$ are highlighted in blue; the set of points for which $(1 + X_1)^2 + (2 - X_2)^2 \le 4$ are highlighted in red.

```{r, out.width = "0.7\\linewidth"}
knitr::include_graphics("C:/Users/George/Desktop/HW11.Q1b.png")
```

## (c)
Suppose that a classifier assigns an observation to the blue class if $$(1 + X_1)^2 + (2 - X_2)^2 > 4,$$ and to the red class otherwise. To what class is the observation (0, 0) classified? (-1, 1)? (2, 2)? (3, 8)?

## Answer
We see from the graphic below that (0, 0), (2, 2), and (3, 8) are assigned to the blue class, while (-1, 1) is assigned to the red class.

```{r, out.width = "0.7\\linewidth"}
knitr::include_graphics("C:/Users/George/Desktop/HW11.Q1c.png")
```

## (d)
Argue that while the decision boundary in (c) is not linear in terms of $X_1$ and $X_2$, it is linear in terms of $X_1$, $X_1^2$, $X_2$, and $X_2^2$.

## Answer
The decision boundary in (c) is not linear in terms of $X_1$ and $X_2$, but if we expand the polynomials, we get 

$$
\begin{aligned}
(1 + X_1)^2 + (2 - X_2)^2 &= 4 \\
1 + 2X_1 + X_1^2 + 4 - 4X_2 + X_2^2 - 4 &= 0 \\
1 + 2X_1 + X_1^2 - 4X_2 + X_2^2 &= 0
\end{aligned}
$$

which is linear in terms of $X_1$, $X_1^2$, $X_2$, and $X_2^2$.

# 2. Question 9.7.7 pg 371
In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set.

## (a)
Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.

## Answer
Using **ifelse()**, we create a binary variable that takes on a value of 1 for cars with *mpg* (gas mileage) above the median and a value of 0 otherwise.

```{r}
# Load the Auto data
data(Auto)

# Create a binary response variable
Auto$mpg <- as.factor(ifelse(Auto$mpg > median(Auto$mpg), 1, 0))
# Drop name
Auto$name <- NULL
```

## (b)
Fit a support vector classifier to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter. Comment on your results.

## Answer
We fit a support vector classifier to the data with various values of cost (0.001, 0.01, 0.1, 1, 5, 10, 100). The response is *mpg*, and the predictors are the other variables (excluding *name*, which is a factor with 304 levels). We use **tune()** from the **e1071** library to perform 10-fold cross-validation on the set of models under consideration. The cross-validation errors for these models can be accessed using **summary()**:

```{r}
set.seed(702)
# Fit a support vector classifier with various values of cost
linear.tune.out <- tune(svm, mpg ~ ., data=Auto, kernel='linear', ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(linear.tune.out)
```

We see that cost=0.01 results in the lowest CV error rate at 0.0867 and cost=0.001 results in the highest CV error rate at 0.135.

The summary of the fitted support vector classifier using cost=0.01 is shown below:

```{r}
# Support vector classifier
linear.svm <- svm(mpg ~ ., data=Auto, kernel='linear', cost=0.01)
summary(linear.svm)
```

We see that there are 166 support vectors, 83 in the 0 class (cars with below median gas mileage) and 83 in the 1 class (cars with above median gas mileage).

## (c)
Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of gamma and degree and cost. Comment on your results.

## Answer
We fit an SVM with radial basis kernels with various values of cost (0.001, 0.01, 0.1, 1, 5, 10, 100) and gamma (0.001, 0.01, 0.1, 1). Like in part (b), we use **tune()** to perform 10-fold cross-validation on the set of models under consideration. The summary is shown below:

```{r}
set.seed(702)
# Fit a radial SVM with various values of gamma and cost
radial.tune.out <- tune(svm, mpg ~ ., data=Auto, kernel='radial', ranges=list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100), 
                                                                       gamma=c(0.001, 0.01, 0.1, 1)))
summary(radial.tune.out)
```

We see that cost=1 and gamma=1 result in the lowest CV error rate at 0.0663.

The summary of the fitted radial SVM using cost=1 and gamma=1 is shown below:

```{r}
# SVM with radial kernel
radial.svm <- svm(mpg ~., data=Auto, kernel='radial', cost=1, gamma=1)
summary(radial.svm)
```

We see that there are 184 support vectors, 92 in the 0 class and 92 in the 1 class.

Next, we fit an SVM with polynomial basis kernels with various values of cost (0.001, 0.01, 0.1, 1, 5, 10, 100) and degree (2, 3, 4, 5). The summary is shown below:

```{r}
set.seed(702)
# Fit a polynomial SVM with various values of degree and cost
poly.tune.out <- tune(svm, mpg ~ ., data=Auto, kernel='polynomial', ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), 
                                                                           degree=2:5))
summary(poly.tune.out)
```

We see that cost=10 and degree=3 result in the lowest CV error rate at 0.0815. 

The summary of the fitted polynomial SVM using cost=10 and degree=3 is shown below:

```{r}
# SVM with polynomial kernel
poly.svm <- svm(mpg ~., data=Auto, kernel='polynomial', cost=10, degree=3)
summary(poly.svm)
```

We see that there are 99 support vectors, 49 in the 0 class and 50 in the 1 class.

## (d)
Make some plots to back up your assertions in (b) and (c).

## Answer
There are too many pairs of predictors to plot in total, so we will just focus on *displacement* vs. *horsepower*, *displacement* vs. *weight*, and *displacement* vs. *acceleration* for each of the SVM's. The region of feature space assigned to the 0 class (cars with below median gas mileage) is shown in light blue, and the region assigned to the 1 class (cars with above median gas mileage) is shown in purple. The support vectors (observations that lie on the margin or on the wrong side of the margin for their class) are plotted as crosses, and the remaining observations are plotted as circles. In parts (b) and (c), we found that there were 166, 184, and 99 support vectors for the SVC, radial SVM, and polynomial SVM, respectively; this is corroborated by the plots in which we see that the polynomial SVM has far fewer support vectors (crosses) than the SVC and radial SVM.

### Support Vector Classifier

```{r}
# Support vector classifier
linear.svm <- svm(mpg ~ ., data=Auto, kernel='linear', cost=0.01)
# displacement vs. horsepower
plot(linear.svm, Auto, displacement~horsepower, slice=list(cylinders=median(Auto$cylinders),                  
                                                           weight=median(Auto$weight), 
                                                           acceleration=median(Auto$acceleration), 
                                                           year=median(Auto$year), 
                                                           origin=median(Auto$origin)))
# displacement vs. weight
plot(linear.svm, Auto, displacement~weight, slice=list(cylinders=median(Auto$cylinders),
                                                       horsepower=median(Auto$horsepower), 
                                                       acceleration=median(Auto$acceleration),
                                                       year=median(Auto$year),
                                                       origin=median(Auto$origin)))
# displacement vs. acceleration
plot(linear.svm, Auto, displacement~acceleration, slice=list(cylinders=median(Auto$cylinders),
                                                             horsepower=median(Auto$horsepower), 
                                                             weight=median(Auto$weight),
                                                             year=median(Auto$year),
                                                             origin=median(Auto$origin)))
```

### SVM with Radial Kernel

```{r}
# displacement vs. horsepower
plot(radial.svm, Auto, displacement~horsepower, slice=list(cylinders=median(Auto$cylinders),                  
                                                           weight=median(Auto$weight), 
                                                           acceleration=median(Auto$acceleration), 
                                                           year=median(Auto$year), 
                                                           origin=median(Auto$origin)))
# displacement vs. weight
plot(radial.svm, Auto, displacement~weight, slice=list(cylinders=median(Auto$cylinders),
                                                       horsepower=median(Auto$horsepower), 
                                                       acceleration=median(Auto$acceleration),
                                                       year=median(Auto$year),
                                                       origin=median(Auto$origin)))
# displacement vs. acceleration
plot(radial.svm, Auto, displacement~acceleration, slice=list(cylinders=median(Auto$cylinders),
                                                             horsepower=median(Auto$horsepower), 
                                                             weight=median(Auto$weight),
                                                             year=median(Auto$year),
                                                             origin=median(Auto$origin)))
```

### SVM with Polynomial Kernel

```{r}
# displacement vs. horsepower
plot(poly.svm, Auto, displacement~horsepower, slice=list(cylinders=median(Auto$cylinders),                  
                                                           weight=median(Auto$weight), 
                                                           acceleration=median(Auto$acceleration), 
                                                           year=median(Auto$year), 
                                                           origin=median(Auto$origin)))
# displacement vs. weight
plot(poly.svm, Auto, displacement~weight, slice=list(cylinders=median(Auto$cylinders),
                                                       horsepower=median(Auto$horsepower), 
                                                       acceleration=median(Auto$acceleration),
                                                       year=median(Auto$year),
                                                       origin=median(Auto$origin)))
# displacement vs. acceleration
plot(poly.svm, Auto, displacement~acceleration, slice=list(cylinders=median(Auto$cylinders),
                                                             horsepower=median(Auto$horsepower), 
                                                             weight=median(Auto$weight),
                                                             year=median(Auto$year),
                                                             origin=median(Auto$origin)))
```

# 3. Question 9.7.8 pg 371
This problem involves the OJ data set which is part of the ISLR package.

## (a)
Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

## Answer
We use **sample()** to divide the **OJ** data into a training set (800 observations) and a test set (270 observations).

```{r}
# Load the OJ data
data(OJ)

set.seed(702)
# Split the data into a training set (800 obs) and a test set (270 obs)
sample <- sample(nrow(OJ), size=800)
train <- OJ[sample, ]
test <- OJ[-sample, ]
```

## (b)
Fit a support vector classifier to the training data using cost=0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained.

## Answer
We fit a support vector classifier to the training data using cost=0.01, with *Purchase* as the response and the other variables as predictors. We use **summary()** to produce summary statistics.

```{r}
# Fit a support vector classifier to the training data
linear.svm <- svm(Purchase ~ ., data=train, kernel='linear', cost=0.01)
summary(linear.svm)
```

The summary tells us that a linear kernel was used with cost=0.01, and that there were 446 support vectors, 224 in the "CH" class and 222 in the "MM" class.

## (c)
What are the training and test error rates?

## Answer
Using a support vector classifier with cost=0.01, the confusion matrices of the training and test sets are shown below:

```{r}
# Predicted training purchases 
pred.train <- linear.svm$fitted
# Confusion matrix of training
table.train <- table(pred.train, train$Purchase, dnn=c('Predicted class', 'True class'))
table.train
# Training error rate
linear.error.train <- 1 - sum(diag(table.train))/sum(table.train)
cat('Training error rate:', linear.error.train, '\n')

# Predicted test purchases
pred.test <- predict(linear.svm, newdata=test)
# Confusion matrix of test
table.test <- table(pred.test, test$Purchase, dnn=c('Predicted class', 'True class'))
table.test
# Test error rate
linear.error.test <- 1 - sum(diag(table.test))/sum(table.test)
cat('Test error rate:', linear.error.test)
```

The training and test error rates are 0.169 and 0.156, respectively.

## (d)
Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.

## Answer
We fit a support vector classifier to the data with various values of cost ($10^{-2}$ to 10, incrementing by power of 0.25). The response is *Purchase*, and the predictors are the other variables. We use **tune()** to perform 10-fold cross-validation on the set of models under consideration. The cross-validation errors for these models can be accessed using **summary()**:

```{r}
set.seed(702)
# Fit a support vector classifier with various values of cost
linear.tune.out <- tune(svm, Purchase ~ ., data=train, kernel='linear', ranges=list(cost=10^seq(-2, 1, by=0.25)))
summary(linear.tune.out)
```

We see that cost=0.562 ($10^{-0.25}$) results in the lowest CV error rate at 0.166.

\newpage

## (e)
Compute the training and test error rates using this new value for cost.

## Answer
Using a support vector classifier with the optimal cost, the confusion matrices of the training and test sets are shown below:

```{r}
# Fit a support vector classifier to the training data using optimal cost
linear.svm <- linear.tune.out$best.model

# Predicted training purchases 
pred.train <- linear.svm$fitted
# Confusion matrix of training
table.train <- table(pred.train, train$Purchase, dnn=c('Predicted class', 'True class'))
table.train
# Training error rate
linear.error.train2 <- 1 - sum(diag(table.train))/sum(table.train)
cat('Training error rate:', linear.error.train2, '\n')

# Predicted test purchases
pred.test <- predict(linear.svm, newdata=test)
# Confusion matrix of test
table.test <- table(pred.test, test$Purchase, dnn=c('Predicted class', 'True class'))
table.test
# Test error rate
linear.error.test2 <- 1 - sum(diag(table.test))/sum(table.test)
cat('Test error rate:', linear.error.test2)
```

We see that the training and test error rates are 0.163 and 0.152, respectively. Compared to the training and test error rates (0.169 and 0.156) using cost=0.01 in part (c), the corresponding error rates using the optimal cost are both lower.

## (f)
Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma.

## Answer
We fit a support vector machine with a radial kernel to the training data using cost=0.01, with *Purchase* as the response and the other variables as predictors. We use **summary()** to produce summary statistics.

```{r}
# Fit a radial SVM to the training data
radial.svm <- svm(Purchase ~ ., data=train, kernel='radial', cost=0.01)
summary(radial.svm)
```

The summary tells us that a radial kernel was used with cost=0.01, and that there were 634 support vectors, 318 in the "CH" class and 316 in the "MM" class.

Using a radial SVM with cost=0.01, the confusion matrices of the training and test sets are shown below:

```{r}
# Predicted training purchases 
pred.train <- radial.svm$fitted
# Confusion matrix of training
table.train <- table(pred.train, train$Purchase, dnn=c('Predicted class', 'True class'))
table.train
# Training error rate
radial.error.train <- 1 - sum(diag(table.train))/sum(table.train)
cat('Training error rate:', radial.error.train, '\n')

# Predicted test purchases
pred.test <- predict(radial.svm, newdata=test)
# Confusion matrix of test
table.test <- table(pred.test, test$Purchase, dnn=c('Predicted class', 'True class'))
table.test
# Test error rate
radial.error.test <- 1 - sum(diag(table.test))/sum(table.test)
cat('Test error rate:', radial.error.test)
```

We see that the training and test error rates are 0.395 and 0.374, respectively.

Next, we fit a radial SVM to the data with various values of cost ($10^{-2}$ to 10, incrementing by power of 0.25). We use **tune()** to perform 10-fold cross-validation on the set of models under consideration. The cross-validation errors for these models can be accessed using **summary()**:

```{r}
set.seed(702)
# Fit a radial SVM with various values of cost
radial.tune.out <- tune(svm, Purchase ~ ., data=train, kernel='radial', ranges=list(cost=10^seq(-2, 1, by=0.25)))
summary(radial.tune.out)
```

We see that cost=3.162 ($10^{0.5}$) results in the lowest CV error rate at 0.180.

Using a radial SVM with the optimal cost, the confusion matrices of the training and test sets are shown below:

```{r}
# Fit a radial SVM to the training data using optimal cost
radial.svm <- radial.tune.out$best.model

# Predicted training purchases 
pred.train <- radial.svm$fitted
# Confusion matrix of training
table.train <- table(pred.train, train$Purchase, dnn=c('Predicted class', 'True class'))
table.train
# Training error rate
radial.error.train2 <- 1 - sum(diag(table.train))/sum(table.train)
cat('Training error rate:', radial.error.train2, '\n')

# Predicted test purchases
pred.test <- predict(radial.svm, newdata=test)
# Confusion matrix of test
table.test <- table(pred.test, test$Purchase, dnn=c('Predicted class', 'True class'))
table.test
# Test error rate
radial.error.test2 <- 1 - sum(diag(table.test))/sum(table.test)
cat('Test error rate:', radial.error.test2)
```

We see that the training and test error rates are 0.144 and 0.148, respectively. Compared to the training and test error rates (0.395 and 0.374) using cost=0.01, the corresponding error rates using the optimal cost are both lower.

## (g)
Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set degree=2.

## Answer
We fit a support vector machine with a polynomial kernel to the training data using cost=0.01 and degree=2, with *Purchase* as the response and the other variables as predictors. We use **summary()** to produce summary statistics.

```{r}
# Fit a polynomial SVM to the training data
poly.svm <- svm(Purchase ~ ., data=train, kernel='polynomial', cost=0.01, degree=2)
summary(poly.svm)
```

The summary tells us that a polynomial kernel was used with cost=0.01 and degree=2, and that there were 635 support vectors, 319 in the "CH" class and 316 in the "MM" class.

Using a polynomial SVM with cost=0.01 and degree=2, the confusion matrices of the training and test sets are shown below:

```{r}
# Predicted training purchases 
pred.train <- poly.svm$fitted
# Confusion matrix of training
table.train <- table(pred.train, train$Purchase, dnn=c('Predicted class', 'True class'))
table.train
# Training error rate
poly.error.train <- 1 - sum(diag(table.train))/sum(table.train)
cat('Training error rate:', poly.error.train, '\n')

# Predicted test purchases
pred.test <- predict(poly.svm, newdata=test)
# Confusion matrix of test
table.test <- table(pred.test, test$Purchase, dnn=c('Predicted class', 'True class'))
table.test
# Test error rate
poly.error.test <- 1 - sum(diag(table.test))/sum(table.test)
cat('Test error rate:', poly.error.test)
```

We see that the training and test error rates are 0.394 and 0.374, respectively.

Next, we fit a polynomial SVM to the data with various values of cost ($10^{-2}$ to 10, incrementing by power of 0.25). We use **tune()** to perform 10-fold cross-validation on the set of models under consideration. The cross-validation errors for these models can be accessed using **summary()**:

```{r}
set.seed(702)
# Fit a polynomial SVM with various values of cost
poly.tune.out <- tune(svm, Purchase ~ ., data=train, kernel='polynomial', ranges=list(cost=10^seq(-2, 1, by=0.25)),
                      degree=2)
summary(poly.tune.out)
```

We see that cost=10 results in the lowest CV error rate at 0.188.

Using a polynomial SVM with the optimal cost, the confusion matrices of the training and test sets are shown below:

```{r}
# Fit a radial SVM to the training data using optimal cost
poly.svm <- poly.tune.out$best.model

# Predicted training purchases 
pred.train <- poly.svm$fitted
# Confusion matrix of training
table.train <- table(pred.train, train$Purchase, dnn=c('Predicted class', 'True class'))
table.train
# Training error rate
poly.error.train2 <- 1 - sum(diag(table.train))/sum(table.train)
cat('Training error rate:', poly.error.train2, '\n')

# Predicted test purchases
pred.test <- predict(poly.svm, newdata=test)
# Confusion matrix of test
table.test <- table(pred.test, test$Purchase, dnn=c('Predicted class', 'True class'))
table.test
# Test error rate
poly.error.test2 <- 1 - sum(diag(table.test))/sum(table.test)
cat('Test error rate:', poly.error.test2)
```

We see that the training and test error rates are 0.159 and 0.167, respectively. Compared to the training and test error rates (0.394 and 0.374) using cost=0.01, the corresponding error rates using the optimal cost are both lower.

## (h)
Overall, which approach seems to give the best results on this data?

## Answer
To compare the results, we create a table of the training and test error rates using cost=0.01 and the optimal cost for each classifier.

```{r}
# Create error vectors for each classifier
linear.errors <- round(c(linear.error.train, linear.error.test, linear.error.train2, linear.error.test2), 4)
radial.errors <- round(c(radial.error.train, radial.error.test, radial.error.train2, radial.error.test2), 4)
poly.errors <- round(c(poly.error.train, poly.error.test, poly.error.train2, poly.error.test2), 4)

# Create a table for comparison
dt <- rbind('SVC'=linear.errors, 'Radial SVM'=radial.errors, 'Polynomial SVM'=poly.errors)
kable(dt, col.names=c('Training (cost=0.01)','Test (cost=0.01)','Training (cost=optimal)','Test (cost=optimal)'),
                      align='c', caption='Training and Test Error Rates Comparison')
```

We see that when cost=0.01, the support vector classifier clearly outperforms the SVM's with radial and polynomial kernels. However, when the optimal cost for each classifier is used, the SVM with a radial kernel produces the lowest training and test error rates. Thus, overall, the SVM with a radial kernel gives the best results on this data. 

# 4. 
In the past couple of homework assignments you have used different classification methods to analyze the dataset you chose. For this homework, use a support vector machine to model your data. Find the test error using any/all of methods (VSA, K-fold CV). Compare the results you obtained with the result from previous homework. Did the results improve? (Use the table with the previous results to compare)

# Answer

## Background Review

I chose the [Adult dataset](https://archive.ics.uci.edu/ml/datasets/Adult) from the UC Irvine Machine Learning Repository. This dataset was extracted from the 1994 Census Bureau database. The task is to classify a person into one of two income groups (>50K or <=50K) based on the predictors using various classification methods and to compare these methods' performances. The full dataset is not provided, but rather separate training and test sets are. There are a total of 48,842 observations (32,561 observations in the training set and 16,281 observations in the test set, i.e. 2/3 training and 1/3 test). There are 15 total variables (response + 14 predictors).

The following numeric variables were chosen as the final predictors through variable selection: *age*, *education_num*, *capital_gain*, *capital_loss*, and *hours_per_week*. To compare the classification methods, we used the validation set approach (VSA), 5-fold and 10-fold cross-validations to estimate the test error for the models. 

```{r}
# Load the Adult data from the website (separate training and test sets are provided)
train <- read.csv(url('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'), header=F)
test <- read.csv(url('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'), header=F, skip=1)

# Add column names
colnames(train) <- colnames(test) <- 
  c("age","workclass","fnlwgt","education","education_num","marital_status",
    "occupation","relationship","race","sex","capital_gain","capital_loss",
    "hours_per_week","native_country","income") 

# Make the income levels of test set consistent with training set
levels(test$income)[1] <- ' <=50K'
levels(test$income)[2] <- ' >50K'

# Full data set
Adult <- rbind(train, test)
```

```{r}
# Function that calculates the misclassification rate, sensitivity, and specificity
classification.function <- function(pred, response){
  # Confusion matrix
  table <- table(pred, response, dnn=c('Predicted class', 'True class'))
  
  # Misclassification rate
  error.rate <- (table[2,1]+table[1,2]) / sum(table)
  # Sensitivity
  sensitivity <- table[2,2] / (table[1,2]+table[2,2])
  # Specificity 
  specificity <- table[1,1] / (table[1,1]+table[2,1])
    
  # Output the misclassification rate, sensitivity, and specificity
  return(c('Test Error Rate'=error.rate, 'Sensitivity'=sensitivity, 'Specificity'=specificity))
}
```

```{r}
# Function that calculates k-fold CV error for LDA, QDA, KNN, Mclust, Mclust with EDDA, ...
# ... tree, bagging, random forests, boosting, and SVC
cv.function <- function(data, k, model){
  set.seed(1)
  # Initialize error vectors
  lda.cv.err <- qda.cv.err <- knn.cv.err <- mclust.cv.err <- mclust2.cv.err <- 
    tree.cv.err <- bag.cv.err <- rf.cv.err <- boost.cv.err <- svc.cv.err <- c()
  # Randomize the order of the data 
  data <- data[sample(nrow(data)), ]
  # Divide the data into k folds of approximately equal size
  folds <- cut(seq(1, nrow(data)), breaks=k, labels=F)
  # Vector of predictor names
  predictors <- c('age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week')
  
  if (model=='lda'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # LDA
      Adult.lda <- lda(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)
      # Predictions of income
      Adult.lda.pred <- predict(Adult.lda, newdata=test, type='response')
      # k-fold CV error
      lda.cv.err[i] <- classification.function(Adult.lda.pred$class, test$income)[1]
    }
    return(mean(lda.cv.err))
  }
  else if (model=='qda'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # QDA
      Adult.qda <- qda(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)
      # Predictions of income
      Adult.qda.pred <- predict(Adult.qda, newdata=test, type='response')
      # k-fold CV error
      qda.cv.err[i] <- classification.function(Adult.qda.pred$class, test$income)[1]
    }
    return(mean(qda.cv.err))
  }
  else if (model=='knn'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # KNN
      train.X <- cbind(train$age, train$education_num, train$capital_gain, train$capital_loss, train$hours_per_week)
      test.X <- cbind(test$age, test$education_num, test$capital_gain, test$capital_loss, test$hours_per_week)
      # Predictions of income
      Adult.knn.pred <- knn(train.X, test.X, train$income, k=sqrt(nrow(train)))
      # k-fold CV error
      knn.cv.err[i] <- classification.function(Adult.knn.pred, test$income)[1]
    }
    return(mean(knn.cv.err))
  }
  else if (model=='mclust'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Mclust
      Adult.mclust <- MclustDA(train[, predictors], train$income)
      # Model summary
      Adult.mclust.summary <- summary(Adult.mclust, newdata=test[, predictors], newclass=test$income)
      # k-fold CV error
      mclust.cv.err[i] <- Adult.mclust.summary$err.newdata
    }
    return(mean(mclust.cv.err))
  }
  else if (model=='mclust2'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Mclust with EDDA
      Adult.mclust2 <- MclustDA(train[, predictors], train$income, modelType='EDDA')
      # Model summary
      Adult.mclust2.summary <- summary(Adult.mclust2, newdata=test[, predictors], newclass=test$income)
      # k-fold CV error
      mclust2.cv.err[i] <- Adult.mclust2.summary$err.newdata
    }
    return(mean(mclust2.cv.err))
  }
  else if (model=='tree'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Classification tree
      Adult.tree <- tree(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)
      # Predictions of income
      Adult.tree.pred <- predict(Adult.tree, newdata=test, type='class')
      # k-fold CV error
      tree.cv.err[i] <- classification.function(Adult.tree.pred, test$income)[1]
    }
    return(mean(tree.cv.err))
  }
  else if (model=='bagging'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Bagging
      Adult.bag <- randomForest(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, 
                                data=train, mtry=5, importance=TRUE)
      # Predictions of income
      Adult.bag.pred <- predict(Adult.bag, newdata=test)
      # k-fold CV error
      bag.cv.err[i] <- classification.function(Adult.bag.pred, test$income)[1]
    }
    return(mean(bag.cv.err))
  }
  else if (model=='randomforest'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Bagging
      Adult.rf <- randomForest(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, 
                                data=train, importance=TRUE)
      # Predictions of income
      Adult.rf.pred <- predict(Adult.rf, newdata=test)
      # k-fold CV error
      rf.cv.err[i] <- classification.function(Adult.rf.pred, test$income)[1]
    }
    return(mean(rf.cv.err))
  }
  else if (model=='boosting'){
    for (i in 1:k){
      # Convert income to 0-1 outcomes (required for gbm classification)
      data$class <- ifelse(data$income == ' <=50K', 0, 1)
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Boosting
      Adult.boost <- gbm(class ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train,
                         distribution='bernoulli', n.trees=500, interaction.depth=4)
      # Predictions probabilities of income
      Adult.boost.probs <- predict(Adult.boost, newdata=test, n.trees=500, type='response')
      # If predicted probability > 0.5, label it as 1; otherwise, label it as 0
      Adult.boost.pred <- rep(0, length(Adult.boost.probs))
      Adult.boost.pred[Adult.boost.probs > 0.5] <- 1
      # k-fold CV error
      boost.cv.err[i] <- classification.function(Adult.boost.pred, test$class)[1]
    }
    return(mean(boost.cv.err))
  }
  else if (model=='svc'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # SVC
      Adult.svc <- svm(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train,
                       kernel='linear', cost=0.01)
      # Predictions of income
      Adult.svc.pred <- predict(Adult.svc, newdata=test)
      # k-fold CV error
      svc.cv.err[i] <- classification.function(Adult.svc.pred, test$income)[1]
      
    }
    return(mean(svc.cv.err))
  }
}
```

```{r}
# Validation set approach (VSA)
# Logistic regression model using training set
Adult.glm <- glm(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train, family=binomial)

# Predicted probabilities of income
Adult.glm.probs <- predict(Adult.glm, newdata=test, type='response')

# If predicted probability > 0.5, label it as ">50K"; otherwise, label it as "<=50K"
Adult.glm.pred <- rep(' <=50K', length(Adult.glm.probs))
Adult.glm.pred[Adult.glm.probs > 0.5] <- ' >50K'

# Validation set error rate
logistic.val.err <- as.vector(classification.function(Adult.glm.pred, test$income)[1])
# cat('VSA Test Error:', logistic.val.err)
```

```{r}
# 5-fold CV
set.seed(1)
# Logistic regression model using full data set
Adult.glm2 <- glm(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=Adult, family=binomial)

# 5-fold CV error
logistic.cv.err <- cv.glm(Adult, Adult.glm2, K=5)$delta[1]
# cat('5-Fold CV Test Error:', logistic.cv.err)
```

```{r}
# 10-fold CV error
set.seed(1)
logistic.cv.err2 <- cv.glm(Adult, Adult.glm2, K=10)$delta[1]
# cat('10-Fold CV Test Error:', logistic.cv.err2)
```

```{r}
# Validation set approach (VSA)
# LDA model using training set
Adult.lda <- lda(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)

# Predictions of income
Adult.lda.pred <- predict(Adult.lda, newdata=test, type='response')

# Validation set error rate
lda.val.err <- as.vector(classification.function(Adult.lda.pred$class, test$income)[1])
# cat('VSA Test Error:', lda.val.err)
```

```{r}
# 5-fold CV
lda.cv.err <- cv.function(data=Adult, k=5, model='lda')
# cat('5-Fold CV Test Error:', lda.cv.err)
```

```{r}
# 10-fold CV
lda.cv.err2 <- cv.function(data=Adult, k=10, model='lda')
# cat('10-Fold CV Test Error:', lda.cv.err2)
```

```{r}
# Validation set approach (VSA)
# QDA model using training set
Adult.qda <- qda(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)

# Predictions of income
Adult.qda.pred <- predict(Adult.qda, newdata=test, type='response')

# Validation set error rate
qda.val.err <- as.vector(classification.function(Adult.qda.pred$class, test$income)[1])
# cat('VSA Test Error:', qda.val.err)
```

```{r}
# 5-fold CV
qda.cv.err <- cv.function(data=Adult, k=5, model='qda')
# cat('5-Fold CV Test Error:', qda.cv.err)
```

```{r}
# 10-fold CV
qda.cv.err2 <- cv.function(data=Adult, k=10, model='qda')
# cat('10-Fold CV Test Error:', qda.cv.err2)
```

```{r}
# Validation set approach (VSA)
# KNN (K = sqrt(n))
set.seed(1)
train.X <- cbind(train$age, train$education_num, train$capital_gain, train$capital_loss, train$hours_per_week)
test.X <- cbind(test$age, test$education_num, test$capital_gain, test$capital_loss, test$hours_per_week)

# Predictions of income
Adult.knn.pred <- knn(train.X, test.X, train$income, k=sqrt(nrow(train)))

# Validation set error rate
knn.val.err <- as.vector(classification.function(Adult.knn.pred, test$income)[1])
# cat('VSA Test Error:', knn.val.err)
```

```{r}
# 5-fold CV
# knn.cv.err <- cv.function(data=Adult, k=5, model='knn')
knn.cv.err <- 0.1698336  # Save result to save time
# cat('5-Fold CV Test Error:', knn.cv.err)
```

```{r}
# 10-fold CV
# knn.cv.err2 <- cv.function(data=Adult, k=10, model='knn')
knn.cv.err2 <- 0.1682774  # Save result to save time
# cat('10-Fold CV Test Error:', knn.cv.err2)
```

```{r}
# Validation set approach (VSA)
# Mclust
set.seed(1)
predictors <- c('age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week')
Adult.mclust <- MclustDA(train[, predictors], train$income)

# Model summary
Adult.mclust.summary <- summary(Adult.mclust, newdata=test[, predictors], newclass=test$income)

# Validation set error rate
mclust.val.err <- Adult.mclust.summary$err.newdata
# cat('VSA Test Error:', mclust.val.err)
```

```{r}
# 5-fold CV
# mclust.cv.err <- cv.function(data=Adult, k=5, model='mclust')
mclust.cv.err <- 0.2525689  # Save the result to save time
# cat('5-Fold CV Test Error:', mclust.cv.err)
```

```{r}
# 10-fold CV
# mclust.cv.err2 <- cv.function(data=Adult, k=10, model='mclust')
mclust.cv.err2 <- 0.2504199 # Save the result to save time
# cat('10-Fold CV Test Error:', mclust.cv.err2)
```

```{r}
# Validation set approach (VSA)
# Mclust with EDDA
set.seed(1)
Adult.mclust2 <- MclustDA(train[, predictors], train$income, modelType='EDDA')

# Model summary
Adult.mclust2.summary <- summary(Adult.mclust2, newdata=test[, predictors], newclass=test$income)

# Validation set error rate
mclust2.val.err <- Adult.mclust2.summary$err.newdata
# cat('VSA Test Error:', mclust2.val.err)
```

```{r}
# 5-fold CV
mclust2.cv.err <- cv.function(data=Adult, k=5, model='mclust2')
# cat('5-Fold CV Test Error:', mclust2.cv.err)
```

```{r}
# 10-fold CV
mclust2.cv.err2 <- cv.function(data=Adult, k=10, model='mclust2')
# cat('10-Fold CV Test Error:', mclust2.cv.err2)
```

```{r}
# Validation set approach (VSA)
# Classification tree
Adult.tree <- tree(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)

# Predictions of income
Adult.tree.pred <- predict(Adult.tree, newdata=test, type='class')

# Validation set error rate
tree.val.err <- as.vector(classification.function(Adult.tree.pred, test$income)[1])
# cat('VSA Test Error:', tree.val.err)
```

```{r}
# 5-fold CV
tree.cv.err <- cv.function(data=Adult, k=5, model='tree')
# cat('5-Fold CV Test Error:', tree.cv.err)
```

```{r}
# 10-fold CV
tree.cv.err2 <- cv.function(data=Adult, k=10, model='tree')
# cat('10-Fold CV Test Error:', tree.cv.err2)
```

```{r}
# Validation set approach (VSA)
# Bagging
set.seed(1)
Adult.bag <- randomForest(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train, 
                          mtry=5, importance=TRUE)

# Predictions of income
Adult.bag.pred <- predict(Adult.bag, newdata=test)

# Validation set error rate
bag.val.err <- as.vector(classification.function(Adult.bag.pred, test$income)[1])
# cat('VSA Test Error:', bag.val.err)
```

```{r}
# 5-fold CV
# bag.cv.err <- cv.function(data=Adult, k=5, model='bagging')
bag.cv.err <- 0.1742149  # Save the result to save time
# cat('5-Fold CV Test Error:', bag.cv.err)
```

```{r}
# 10-fold CV
# bag.cv.err2 <- cv.function(data=Adult, k=10, model='bagging')
bag.cv.err2 <- 0.173007  # Save the result to save time
# cat('10-Fold CV Test Error:', bag.cv.err2)
```

```{r}
# Validation set approach (VSA)
# Random forests
set.seed(1)
Adult.rf <- randomForest(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train,
                         importance=TRUE)

# Predictions of income
Adult.rf.pred <- predict(Adult.rf, newdata=test)

# Validation set error rate
rf.val.err <- as.vector(classification.function(Adult.rf.pred, test$income)[1])
# cat('VSA Test Error:', rf.val.err)
```

```{r}
# 5-fold CV
# rf.cv.err <- cv.function(data=Adult, k=5, model='randomforest')
rf.cv.err <- 0.1609682  # Save the result to save time
# cat('5-Fold CV Test Error:', rf.cv.err)
```

```{r}
# 10-fold CV
# rf.cv.err2 <- cv.function(data=Adult, k=10, model='randomforest')
rf.cv.err2 <- 0.1600673  # Save the result to save time
# cat('10-Fold CV Test Error:', rf.cv.err2)
```

```{r}
# Validation set approach (VSA)
# Convert income to 0-1 outcomes (required for gbm classification)
train$class <- ifelse(train$income == ' <=50K', 0, 1)
test$class <- ifelse(test$income == ' <=50K', 0, 1)
# Boosting
set.seed(1)
Adult.boost <- gbm(class ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train, 
                   distribution='bernoulli', n.trees=500, interaction.depth=4)

# Predictions probabilities of income
Adult.boost.probs <- predict(Adult.boost, newdata=test, n.trees=500, type='response')

# If predicted probability > 0.5, label it as 1; otherwise, label it as 0
Adult.boost.pred <- rep(0, length(Adult.boost.probs))
Adult.boost.pred[Adult.boost.probs > 0.5] <- 1

# Validation set error rate
boost.val.err <- as.vector(classification.function(Adult.boost.pred, test$class)[1])
# cat('VSA Test Error:', boost.val.err)
```

```{r}
# 5-fold CV
# boost.cv.err <- cv.function(data=Adult, k=5, model='boosting')
boost.cv.err <- 0.1569348  # Save result to save time
# cat('5-Fold CV Test Error:', boost.cv.err)
```

```{r}
# 10-fold CV
# boost.cv.err2 <- cv.function(data=Adult, k=10, model='boosting')
boost.cv.err2 <- 0.1563819  # Save result to save time
# cat('10-Fold CV Test Error:', boost.cv.err2)
```

## Summary of Previous Classification Methods

```{r}
# Vector of model names
model.vector <- c('Logistic Regression', 'LDA', 'QDA', 'KNN (K=sqrt(n))', 'MclustDA', 
                  'MclustDA with EDDA', 'Classification Tree', 'Bagging', 'Random Forests',
                  'Boosting')

# Vectors of test error rates 
logistic.err <- c(logistic.val.err, logistic.cv.err, logistic.cv.err2)
lda.err <- c(lda.val.err, lda.cv.err, lda.cv.err2)
qda.err <- c(qda.val.err, qda.cv.err, qda.cv.err2)
knn.err <- c(knn.val.err, knn.cv.err, knn.cv.err2)
mclust.err <- c(mclust.val.err, mclust.cv.err, mclust.cv.err2)
mclust2.err <- c(mclust2.val.err, mclust2.cv.err, mclust2.cv.err2)
tree.err <- c(tree.val.err, tree.cv.err, tree.cv.err2)
bag.err <- c(bag.val.err, bag.cv.err, bag.cv.err2)
rf.err <- c(rf.val.err, rf.cv.err, rf.cv.err2)
boost.err <- c(boost.val.err, boost.cv.err, boost.cv.err2)

# Matrix of test error rates
err.matrix <- round(rbind(logistic.err, lda.err, qda.err, knn.err, 
                          mclust.err, mclust2.err, tree.err, bag.err,
                          rf.err, boost.err), 4)
rownames(err.matrix) <- NULL
colnames(err.matrix) <- c('VSA', '5-Fold CV', '10-Fold CV')

dt <- cbind(Method=model.vector, err.matrix)
kable(dt, align='l', caption='Test Error Rate Comparison for the Classification Methods')
```

**Table 2** shows the VSA, 5-fold, and 10-fold CV test errors associated with each model.

\newpage

5-fold or 10-fold CV are more reliable than VSA since their test error rate estimates suffer neither from high bias nor high variance, while the estimate using VSA is highly variable. Logistic regression performed the best since it has the lowest 5-fold and 10-fold CV test errors, although boosting did have the best VSA test error. On the other hand, MclustDA by far performed the worst for all three cross-validation methods.

## Support Vector Classifier
We fit a support vector classifier to the training data using cost=0.01. We use **summary()** to produce summary statistics.

```{r}
# Fit a support vector classifier to the training data
Adult.svc <- svm(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train, 
                  kernel='linear', cost=0.01)
summary(Adult.svc)
```

The summary tells us that a linear kernel was used with cost=0.01, and that there were 13,751 support vectors, 6,877 in the <=50K income group and 6,874 in the >50K income group.

The VSA, 5-fold, and 10-fold CV test errors are calculated below:

```{r}
# Validation set approach (VSA)
# Predictions of income
Adult.svc.pred <- predict(Adult.svc, newdata=test)

# Validation set error rate
svc.val.err <- as.vector(classification.function(Adult.svc.pred, test$income)[1])
cat('VSA Test Error:', svc.val.err)
```

```{r}
# 5-fold CV
# svc.cv.err <- cv.function(data=Adult, k=5, model='svc')
svc.cv.err <- 0.2002582  # Save result to save time
cat('5-Fold CV Test Error:', svc.cv.err)
```

```{r}
# 10-fold CV
# svc.cv.err2 <- cv.function(data=Adult, k=10, model='svc')
svc.cv.err2 <- 0.2008314  # Save result to save time
cat('10-Fold CV Test Error:', svc.cv.err2)
```

\newpage

## Summary of Classification Methods Including SVC

```{r}
# Vector of model names
model.vector <- c('Logistic Regression', 'LDA', 'QDA', 'KNN (K=sqrt(n))', 'MclustDA', 
                  'MclustDA with EDDA', 'Classification Tree', 'Bagging', 'Random Forests',
                  'Boosting', 'SVC')

# Vectors of test error rates 
logistic.err <- c(logistic.val.err, logistic.cv.err, logistic.cv.err2)
lda.err <- c(lda.val.err, lda.cv.err, lda.cv.err2)
qda.err <- c(qda.val.err, qda.cv.err, qda.cv.err2)
knn.err <- c(knn.val.err, knn.cv.err, knn.cv.err2)
mclust.err <- c(mclust.val.err, mclust.cv.err, mclust.cv.err2)
mclust2.err <- c(mclust2.val.err, mclust2.cv.err, mclust2.cv.err2)
tree.err <- c(tree.val.err, tree.cv.err, tree.cv.err2)
bag.err <- c(bag.val.err, bag.cv.err, bag.cv.err2)
rf.err <- c(rf.val.err, rf.cv.err, rf.cv.err2)
boost.err <- c(boost.val.err, boost.cv.err, boost.cv.err2)
svc.err <- c(svc.val.err, svc.cv.err, svc.cv.err2)

# Matrix of test error rates
err.matrix <- round(rbind(logistic.err, lda.err, qda.err, knn.err, 
                          mclust.err, mclust2.err, tree.err, bag.err,
                          rf.err, boost.err, svc.err), 4)
rownames(err.matrix) <- NULL
colnames(err.matrix) <- c('VSA', '5-Fold CV', '10-Fold CV')

dt <- cbind(Method=model.vector, err.matrix)
kable(dt, align='l', caption='Test Error Rate Comparison for the Classification Methods')
```

**Table 3** shows the VSA, 5-fold, and 10-fold CV test errors associated with each model including the support vector classifier.

We see that SVC does not perform very well on this data, and logistic regression is still the best in terms of 5-fold and 10-fold CV test errors.