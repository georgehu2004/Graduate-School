---
title: "STAT 602 Homework 10"
author: "Yuchi Hu"
date: "March 28, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=F,warning=F,echo=F,fig.height=4,fig.width=6,cache=F,out.extra='',fig.pos='h')
```

```{r, eval=F}
# Install packages used in this homework
install.packages(ISLR)
install.packages(tree)
install.packages(maptree)
install.packages(ggplot2)
install.packages(ggdendro)
install.packages(randomForest)
install.packages(gbm)
install.packages(glmnet)
install.packages(boot)
install.packages(MASS)
install.packages(class)
install.packages(mclust)
install.packages(knitr)
```

```{r}
# Load packages used in this homework
library(ISLR)
library(tree)
library(maptree)
library(ggplot2)
library(ggdendro)
library(randomForest)
library(gbm)
library(glmnet)
library(boot)
library(MASS)
library(class)
library(mclust)
library(knitr)
```

# 1. Question 8.4.4 pg 332
This question relates to the plots in Figure 8.12.

## (a)
Sketch the tree corresponding to the partition of the predictor space illustrated in the left-hand panel of Figure 8.12. The numbers inside the boxes indicate the mean of Y within each region.

## Answer
The rectangles indicate splits, and the circles are terminal nodes.  The number inside each node is the mean of the response for observations that fall there.

```{r, out.width = "0.8\\linewidth"}
knitr::include_graphics("C:/Users/George/Desktop/HW10.Q1a.png")
```

\newpage

## (b)
Create a diagram similar to the left-hand panel of Figure 8.12, using the tree illustrated in the right-hand panel of the same figure. You should divide up the predictor space into the correct regions, and indicate the mean for each region.

## Answer
The predictor space is partitioned into five regions corresponding to the five terminal nodes.  The number inside each region is the mean of the response for observations that fall there.

```{r, out.width = "0.8\\linewidth"}
knitr::include_graphics("C:/Users/George/Desktop/HW10.Q1b.png")
```

# 2. Question 8.4.8 pg 332
In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.

## (a)
Split the data set into a training set and a test set.

## Answer
We use **sample()** with an 80:20 split to divide the **Carseats** data into a training set (320 observations) and a test set (80 observations).

```{r}
# Load the Carseats data
data(Carseats)

set.seed(702)
# Split the data into a training set and a test set (80:20)
sample <- sample(nrow(Carseats), size=0.8*nrow(Carseats))
train <- Carseats[sample, ]
test <- Carseats[-sample, ]
```

## (b)
Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?

## Answer
We fit a regression tree to the training set using **tree()** from the **tree** package. The response is *Sales*, and the predictors are the other variables. We plot the tree using **draw.tree()** from the **maptree** package. For the ggplot2 equivalent, we use the **ggdendro** package.

```{r,fig.height=7,fig.width=11}
# Fit a regression tree to training set
Carseats.tree <- tree(Sales ~ ., data=train)

# Plot the tree
draw.tree(Carseats.tree, cex=0.8, size=2, nodeinfo=F, digits=2, print.levels=T, new=T)
title(main='Regression Tree for the Carseats Data')
```

```{r, fig.height=5, fig.width=8}
# ggplot2 (using ggdendro package)
Carseats.treer <- dendro_data(Carseats.tree)
theme_update(plot.title=element_text(hjust=0.5))
ggplot() + geom_segment(data=Carseats.treer$segments, aes(x=x, y=y, xend=xend, yend=yend)) + 
  geom_text(data=Carseats.treer$labels, aes(x=x, y=y, label=label), size=4, vjust=0) +
  geom_text(data=Carseats.treer$leaf_labels, aes(x=x, y=y, label=label), size=4, vjust=1) + theme_dendro() + 
  labs(title='GGPLOT2: Regression Tree for the Carseats Data')
```

We see that there are 18 terminal nodes or leaves. The top split assigns observations with "Bad" or "Medium" *ShelveLoc* to the left branch (ac corresponds to the first and third factor levels of *ShelveLoc*), and then that group is further split on *Price* = 105.5 and so on. Observations with "Good" *ShelveLoc* are assigned to the right branch, and then further split on *Price* = 109.5 and so on. Thus, *ShelveLoc* (quality of the shelving location) is the most important factor in determining *Sales*. Overall, the tree segments the observations into 18 regions of predictor space.

```{r}
# Predicted test sales
pred <- predict(Carseats.tree, newdata=test)
# Test MSE
cat('Test MSE:', mean((test$Sales - pred)^2))
```

We obtain a test MSE of 5.314.

## (c)
Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?

## Answer
We use **cv.tree()** with 10-fold cross-validation to determine the optimal level of tree complexity. The optimal tree size corresponds to that with the lowest cross-validation error or deviance.

```{r}
set.seed(702)
# Use cross-validation to determine optimal tree size
Carseats.cv <- cv.tree(Carseats.tree)
opt.size <- Carseats.cv$size[which.min(Carseats.cv$dev)]
cat('Optimal size:', opt.size)
```

Thus, the optimal level of tree complexity is 10 terminal nodes. The resulting pruned tree is shown below:

```{r, fig.height=5, fig.width=7}
# Prune the tree
Carseats.prune <- prune.tree(Carseats.tree, best=10)

# Plot the pruned tree
draw.tree(Carseats.prune, cex=0.8, size=2, nodeinfo=F, digits=2, print.levels=T, new=T)
title(main='Pruned Regression Tree for the Carseats Data')
```

```{r}
# ggplot2 (using ggdendro package)
Carseats.pruner <- dendro_data(Carseats.prune)
ggplot() + geom_segment(data=Carseats.pruner$segments, aes(x=x, y=y, xend=xend, yend=yend)) + 
  geom_text(data=Carseats.pruner$labels, aes(x=x, y=y, label=label), size=4, vjust=0) +
  geom_text(data=Carseats.pruner$leaf_labels, aes(x=x, y=y, label=label), size=4, vjust=1) + theme_dendro() + 
  labs(title='GGPLOT2: Pruned Regression Tree for the Carseats Data')
```

```{r}
# Predicted test sales
pred <- predict(Carseats.prune, newdata=test)
# Test MSE
cat('Test MSE:', mean((test$Sales - pred)^2))
```

The test MSE is 5.826. Since the test MSE associated with the unpruned tree is 5.314, pruning the tree does not improve the test MSE.

## (d)
Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.

## Answer
We use **randomForest()** from the **randomForest** package to perform bagging. Bagging is simply random forest with *m = p*, so **randomForest()** can be used to perform both. The argument mtry=10 indicates that all 10 predictors should be considered for each split of the tree. By default, 500 trees are grown.

```{r}
set.seed(702)
# Bagging
Carseats.bag <- randomForest(Sales ~ ., data=train, mtry=10, importance=TRUE)

# Predicted test sales
pred <- predict(Carseats.bag, newdata=test)
# Test MSE
cat('Test MSE:', mean((test$Sales - pred)^2))
```

We obtain a test MSE of 3.343. 

Next, we use **importance()** to view the importance of each variable.

```{r}
# Variable importance
importance(Carseats.bag)
```

The first measure (%IncMSE) is the mean decrease of accuracy in predictions on the out of bag samples when a given variable is excluded from the model. The second measure (IncNodePurity) is the total decrease in node impurity that results from splits over that variable, averaged over all trees. We see that *ShelveLoc* and *Price* are by far the two most important variables.

## (e)
Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.

## Answer
Similar to bagging, We use **randomForest()** to perform random forests. In random forests, only a random subset of the predictors is considered for each split; by default, **randomForest()** uses $m \approx p/3$ when building a random forest of regression trees. In this case, *m* = 3 since there are 10 predictors. 

```{r}
set.seed(702)
# Random forests
Carseats.rf <- randomForest(Sales ~ ., data=train, importance=TRUE)

# Predicted test sales
pred <- predict(Carseats.rf, newdata=test)
# Test MSE
cat('Test MSE:', mean((test$Sales - pred)^2))
```

We obtain a test MSE of 3.665. Since the test MSE associated with bagging is 3.343, random forest does not improve upon bagging in this case.

Next, we use **importance()** to view the importance of each variable.

```{r}
# Variable importance
importance(Carseats.rf)
```

Again, we see that *ShelveLoc* and *Price* are by far the two most important variables.

# 3. Question 8.4.9 pg 334
This problem involves the OJ data set which is part of the ISLR package.

## (a)
Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

## Answer
We use **sample()** to divide the **OJ** data into a training set (800 observations) and a test set (270 observations).

```{r}
# Load the OJ data
data(OJ)

set.seed(702)
# Split the data into a training set (800 obs) and a test set (270 obs)
sample <- sample(nrow(OJ), size=800)
train <- OJ[sample, ]
test <- OJ[-sample, ]
```

## (b)
Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?

## Answer
We fit a classification tree to the training set with *Purchase* as the response and the other variables as predictors. We use **summary()** to produce summary statistics about the tree.

```{r}
# Fit a regression tree to training set
OJ.tree <- tree(Purchase ~ ., data=train)
# Summary
summary(OJ.tree)
```

We see that only two out of the 17 predictors were used in tree construction: *LoyalCH* and *PriceDiff*. The training error rate is 0.1662. The tree has 8 terminal nodes.

## (c)
Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.

## Answer
We type in the name of the tree object in order to get a detailed text ouput.

```{r}
# Output for the tree object
OJ.tree
```

Branches that lead to terminal nodes are indicated using asterisks. If we look at node 8, we see that the split criterion is *LoyalCH* < 0.0513. The number of observations in that branch is 60, the deviance is 10.17, and the overall prediction for the branch is "MM". Finally, the fraction of observations in that branch that take on values of "CH" and "MM" are 0.0167 and 0.983, respectively.

## (d)
Create a plot of the tree, and interpret the results.

## Answer
The plot of the tree is shown below:

```{r}
# Plot the tree
draw.tree(OJ.tree, cex=0.8, size=2, nodeinfo=F, digits=2, print.levels=T, new=T)
title(main='Classification Tree for the OJ Data')
```

```{r, fig.height=3, fig.width=5}
# ggplot2 (using ggdendro package)
OJ.treer <- dendro_data(OJ.tree)
ggplot() + geom_segment(data=OJ.treer$segments, aes(x=x, y=y, xend=xend, yend=yend)) + 
  geom_text(data=OJ.treer$labels, aes(x=x, y=y, label=label), size=4, vjust=0) +
  geom_text(data=OJ.treer$leaf_labels, aes(x=x, y=y, label=label), size=4, vjust=1) + theme_dendro() + 
  labs(title='GGPLOT2: Classification Tree for the OJ Data')
```

We see that there are 8 terminal nodes or leaves. Only *LoyalCH* and *PriceDiff* were used in tree construction. The top three splits occur on *LoyalCH*; thus, *LoyalCH* is the most important factor in determining *Purchase*. Overall, the tree segments the observations into 8 regions of predictor space.

\newpage

## (e)
Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?

## Answer
We predict the response on the test data and produce a confusion matrix comparing the test labels to the predicted test labels.

```{r}
# Predicted test purchases
pred <- predict(OJ.tree, newdata=test, type='class')
# Confusion matrix
table <- table(pred, test$Purchase, dnn=c('Predicted class', 'True class'))
table
# Test error rate
error <- 1 - sum(diag(table))/sum(table)
cat('Test error rate:', error)
```

The test error rate is 0.156.

## (f)
Apply the cv.tree() function to the training set in order to determine the optimal tree size.

## Answer
We apply **cv.tree()** to the training set in order to determine the optimal tree size. By default, 10-fold CV is performed.

```{r}
set.seed(702)
# Use cross-validation to determine optimal tree size
OJ.cv <- cv.tree(OJ.tree, FUN=prune.misclass)
OJ.cv
```

"dev" corresponds to the cross-validation error rate. We see that the trees with 8 and 7 terminal nodes are tied for the lowest CV error rate, with 159 CV errors each. We choose the smaller of the two (7 terminal nodes) as the optimal tree size.

\newpage

## (g)
Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.

## Answer
We produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.

```{r}
# CV error rate vs. tree size
plot(OJ.cv$size, OJ.cv$dev, type='b', main='CV Error Rate vs. Tree Size', ylab='CV Error Rate', xlab='Tree Size')
```

```{r, fig.height=3, fig.width=5}
# ggplot2
ggplot(data=NULL, aes(x=OJ.cv$size, y=OJ.cv$dev)) + geom_line() + geom_point() +
  labs(title='GGPLOT2: CV Error Rate vs. Tree Size', y='CV Error Rate', x='Tree Size')
```

## (h)
Which tree size corresponds to the lowest cross-validated classification error rate?

## Answer
The tree with 7 or 8 terminal nodes corresponds to the lowest cross-validated classification error rate.

## (i)
Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.

## Answer
The plot of the pruned tree with 7 terminal nodes is shown below:

```{r}
# Prune the tree
OJ.prune <- prune.misclass(OJ.tree, best=7)

# Plot the tree
draw.tree(OJ.prune, cex=0.8, size=2, nodeinfo=F, digits=2, print.levels=T, new=T)
title(main='Pruned classification Tree for the OJ Data')
```

```{r, fig.height=3, fig.width=5}
# ggplot2 (using ggdendro package)
OJ.pruner <- dendro_data(OJ.prune)
ggplot() + geom_segment(data=OJ.pruner$segments, aes(x=x, y=y, xend=xend, yend=yend)) + 
  geom_text(data=OJ.pruner$labels, aes(x=x, y=y, label=label), size=4, vjust=0) +
  geom_text(data=OJ.pruner$leaf_labels, aes(x=x, y=y, label=label), size=4, vjust=1) + theme_dendro() + 
  labs(title='GGPLOT2: Pruned Classification Tree for the OJ Data')
```

## (j)
 Compare the training error rates between the pruned and unpruned trees. Which is higher?

## Answer
The summary of the unpruned tree is shown below:

```{r}
# Summary of unpruned tree
summary(OJ.tree)
```

The training error rate of the unpruned tree is 0.1662.

The summary of the pruned tree is shown below:

```{r}
# Summary of pruned tree
summary(OJ.prune)
```

The training error rate of the pruned tree is 0.1662. Thus, the training error rates of the pruned and unpruned trees are equal.

## (k)
Compare the test error rates between the pruned and unpruned trees. Which is higher?

## Answer
The confusion matrices of the unpruned and pruned trees are shown below:

```{r}
# Predicted test purchases of unpruned tree
pred.unprune <- predict(OJ.tree, newdata=test, type='class')
# Predicted test purchases of pruned tree
pred.prune <- predict(OJ.prune, newdata=test, type='class')

# Confusion matrix of unpruned tree
table.unprune <- table(pred.unprune, test$Purchase, dnn=c('Predicted class', 'True class'))
table.unprune
# Test error rate of unpruned tree
error.unprune <- 1 - sum(diag(table.unprune))/sum(table.unprune)
cat('Test error rate of unpruned tree:', error.unprune, '\n')

# Confusion matrix of pruned tree
table.prune <- table(pred.prune, test$Purchase, dnn=c('Predicted class', 'True class'))
table.prune
# Test error rate of pruned tree
error.prune <- 1 - sum(diag(table.prune))/sum(table.prune)
cat('Test error rate of pruned tree:', error.prune)
```

We see that the confusion matrices and test error rates of the unpruned (8 terminal nodes) and pruned (7 terminal nodes) trees are identical. Both test error rates are 0.156.

# 4. Question 8.4.10 pg 334
We now use boosting to predict Salary in the Hitters data set.

## (a)
Remove the observations for whom the salary information is unknown, and then log-transform the salaries.

## Answer
We remove the observations for whom the salary information is unknown, and then log-transform the salaries. There are 59 observations with unknown salary information.

```{r}
# Load the Hitters data
data(Hitters)

# Remove unknown Salary
Hitters <- Hitters[!is.na(Hitters$Salary),]
# Log-transform Salary
Hitters$Salary <- log(Hitters$Salary)
```

## (b)
Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.

## Answer
We create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.

```{r}
# Split the data into a training set (first 200 obs) and a test set (remaining obs)
sample <- 1:200
train <- Hitters[sample,]
test <- Hitters[-sample,] 
```

## (c)
Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter $\lambda$. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.

## Answer
We perform boosting on the training set with 1,000 trees for shrinkage values from 0.001 to 0.1, incrementing by 0.005. Then, we produce a plot with shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.

```{r}
set.seed(702)
# Range of lambdas
lambdas <- seq(from=0.001, to=0.1, by=0.005)
# Initialize MSE vector
MSE <- c()
for (i in 1:length(lambdas)){
  # Perform boosting on the training set
  Hitters.boost <- gbm(Salary ~ ., data=train, distribution='gaussian', n.trees=1000, 
                       interaction.depth=4, shrinkage=lambdas[i])
  # Predicted training salaries
  pred <- Hitters.boost$fit
  # Training MSE
  MSE[i] <- mean((train$Salary - pred)^2)
}
```

```{r}
# Training MSE vs. lambda
plot(lambdas, MSE, type='b', main='Training MSE vs. Lambda', ylab='Training MSE', xlab='Lambda')
```

```{r, fig.height=3, fig.width=5}
# ggplot2
ggplot(data=NULL, aes(x=lambdas, y=MSE)) + geom_line() + geom_point() +
  labs(title='GGPLOT2: Training MSE vs. Lambda', y='Training MSE', x='Lambda')
```

## (d)
Produce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis.

## Answer
We produce a plot with shrinkage values on the x-axis and the corresponding test set MSE on the y-axis. The shrinkage values range from 0.001 to 0.1, incrementing by 0.005 (same as part(c)).

```{r}
set.seed(702)
# Range of lambdas
lambdas <- seq(from=0.001, to=0.1, by=0.005)
# Initialize MSE vector
MSE <- c()
for (i in 1:length(lambdas)){
  # Perform boosting on the training set
  Hitters.boost <- gbm(Salary ~ ., data=train, distribution='gaussian', n.trees=1000, 
                       interaction.depth=4, shrinkage=lambdas[i])
  # Predicted test salaries
  pred <- predict(Hitters.boost, newdata=test, n.trees=1000)
  # Test MSE
  MSE[i] <- mean((test$Salary - pred)^2)
}
```

```{r}
# Test MSE vs. lambda
plot(lambdas, MSE, type='b', main='Test MSE vs. Lambda', ylab='Test MSE', xlab='Lambda')
```

```{r, fig.height=3, fig.width=5}
# ggplot2
ggplot(data=NULL, aes(x=lambdas, y=MSE)) + geom_line() + geom_point() +
  labs(title='GGPLOT2: Test MSE vs. Lambda', y='Test MSE', x='Lambda')
```

## (e)
Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6.

## Answer
First, we need to find the lowest test MSE of boosting and the associated $\lambda$.

```{r}
# Lowest test MSE of boosting and best lambda
cat('Lowest test MSE of boosting:', min(MSE), '\n')
cat('Best lambda for boosting:', lambdas[which.min(MSE)])
```

The lowest test MSE of boosting is 0.266, and the associated $\lambda$ is 0.036.

Next, we calculate the test MSE of linear regression (chapter 3) and ridge regression (chapter 6).

```{r}
# Fit a linear regression model on the training set
Hitters.lm <- lm(Salary ~ ., data=train)
# Predicted test salaries
pred <- predict(Hitters.lm, newdata=test)
# Test MSE
MSE.lm <- mean((test$Salary - pred)^2)
cat('Test MSE of linear regression:', MSE.lm, '\n')

# Fit a ridge regression model on the training set
# Matrices of predictors
train.x <- model.matrix(Salary ~ ., data=train)[,-1]
test.x <- model.matrix(Salary ~., data=test)[,-1]
grid <- 10^seq(10, -2, length=100)
Hitters.ridge <- glmnet(train.x, train$Salary, alpha=0, lambda=grid, thresh=1e-12)
set.seed(702)
# Cross-validation
cv.out <- cv.glmnet(train.x, train$Salary, alpha=0)
# Lambda that results in smallest CV error
bestlam <- cv.out$lambda.min
# Predicted test salaries
pred <- predict(Hitters.ridge, s=bestlam, newx=test.x)
# Test MSE
MSE.ridge <- mean((test$Salary - pred)^2)
cat('Test MSE of ridge regression:', MSE.ridge)

```

The test MSE's of linear and ridge regressions are 0.492 and 0.453, respectively. Thus, the test MSE of boosting (0.266) is lower than the test MSE's of linear and ridge regressions.

## (f)
Which variables appear to be the most important predictors in the boosted model?

## Answer
We perform boosting on the training set with 1,000 trees and the shrinkage value (0.036) associated with the lowest test MSE (see part (e)). The boosted model summary outputs relative influence statstics: 

```{r}
set.seed(702)
# Perform boosting on the training set with lambda from part (e)
Hitters.boost <- gbm(Salary ~ ., data=train, distribution='gaussian', n.trees=1000, 
                       interaction.depth=4, shrinkage=lambdas[which.min(MSE)])
# Boosting summary
summary(Hitters.boost, plotit=FALSE)
```

We see that *CAtBat* (number of career at bats) is by far the most important variable in predicting *Salary*. 

## (g)
Now apply bagging to the training set. What is the test set MSE for this approach?

## Answer
We apply bagging to the training set.

```{r}
set.seed(702)
# Bagging
Hitters.bag <- randomForest(Salary ~ ., data=train, mtry=19, importance=TRUE)

# Predicted test salaries
pred <- predict(Hitters.bag, newdata=test)
# Test MSE
cat('Test MSE:', mean((test$Salary - pred)^2))
```

The test MSE is 0.231. This is even better than the test MSE of boosting (0.266).

# 5. 
In the past couple of homework assignments you have used different classification methods to analyze the dataset you chose. For this homework, use tree-based classification methods (tree,bagging, randomforest, boosting) to model your data. Find the test error using any/all of methods (VSA, K-fold CV). Compare the results you obtained with the result from previous homework. Did the results improve? (Use the table you previously made to compare results)

# Answer

## Background Review

I chose the [Adult dataset](https://archive.ics.uci.edu/ml/datasets/Adult) from the UC Irvine Machine Learning Repository. This dataset was extracted from the 1994 Census Bureau database. The task is to classify a person into one of two income groups (>50K or <=50K) based on the predictors using various classification methods and to compare these methods' performances. The full dataset is not provided, but rather separate training and test sets are. There are a total of 48,842 observations (32,561 observations in the training set and 16,281 observations in the test set, i.e. 2/3 training and 1/3 test). There are 15 total variables (response + 14 predictors).

The following numeric variables were chosen as the final predictors through variable selection: *age*, *education_num*, *capital_gain*, *capital_loss*, and *hours_per_week*. To compare the classification methods, we used the validation set approach (VSA), 5-fold and 10-fold cross-validations to estimate the test error for the models. 

```{r}
# Load the Adult data from the website (separate training and test sets are provided)
train <- read.csv(url('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'), header=F)
test <- read.csv(url('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'), header=F, skip=1)

# Add column names
colnames(train) <- colnames(test) <- 
  c("age","workclass","fnlwgt","education","education_num","marital_status",
    "occupation","relationship","race","sex","capital_gain","capital_loss",
    "hours_per_week","native_country","income") 

# Make the income levels of test set consistent with training set
levels(test$income)[1] <- ' <=50K'
levels(test$income)[2] <- ' >50K'

# Full data set
Adult <- rbind(train, test)
```

```{r}
# Function that calculates the misclassification rate, sensitivity, and specificity
classification.function <- function(pred, response){
  # Confusion matrix
  table <- table(pred, response, dnn=c('Predicted class', 'True class'))
  
  # Misclassification rate
  error.rate <- (table[2,1]+table[1,2]) / sum(table)
  # Sensitivity
  sensitivity <- table[2,2] / (table[1,2]+table[2,2])
  # Specificity 
  specificity <- table[1,1] / (table[1,1]+table[2,1])
    
  # Output the misclassification rate, sensitivity, and specificity
  return(c('Test Error Rate'=error.rate, 'Sensitivity'=sensitivity, 'Specificity'=specificity))
}
```

```{r}
# Function that calculates k-fold CV error for LDA, QDA, KNN, Mclust, Mclust with EDDA, ...
# ... tree, bagging, random forests, and boosting
cv.function <- function(data, k, model){
  set.seed(1)
  # Initialize error vectors
  lda.cv.err <- qda.cv.err <- knn.cv.err <- mclust.cv.err <- mclust2.cv.err <- 
    tree.cv.err <- bag.cv.err <- rf.cv.err <- boost.cv.err <- c()
  # Randomize the order of the data 
  data <- data[sample(nrow(data)), ]
  # Divide the data into k folds of approximately equal size
  folds <- cut(seq(1, nrow(data)), breaks=k, labels=F)
  # Vector of predictor names
  predictors <- c('age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week')
  
  if (model=='lda'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # LDA
      Adult.lda <- lda(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)
      # Predictions of income
      Adult.lda.pred <- predict(Adult.lda, newdata=test, type='response')
      # k-fold CV error
      lda.cv.err[i] <- classification.function(Adult.lda.pred$class, test$income)[1]
    }
    return(mean(lda.cv.err))
  }
  else if (model=='qda'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # QDA
      Adult.qda <- qda(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)
      # Predictions of income
      Adult.qda.pred <- predict(Adult.qda, newdata=test, type='response')
      # k-fold CV error
      qda.cv.err[i] <- classification.function(Adult.qda.pred$class, test$income)[1]
    }
    return(mean(qda.cv.err))
  }
  else if (model=='knn'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # KNN
      train.X <- cbind(train$age, train$education_num, train$capital_gain, train$capital_loss, train$hours_per_week)
      test.X <- cbind(test$age, test$education_num, test$capital_gain, test$capital_loss, test$hours_per_week)
      # Predictions of income
      Adult.knn.pred <- knn(train.X, test.X, train$income, k=sqrt(nrow(train)))
      # k-fold CV error
      knn.cv.err[i] <- classification.function(Adult.knn.pred, test$income)[1]
    }
    return(mean(knn.cv.err))
  }
  else if (model=='mclust'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Mclust
      Adult.mclust <- MclustDA(train[, predictors], train$income)
      # Model summary
      Adult.mclust.summary <- summary(Adult.mclust, newdata=test[, predictors], newclass=test$income)
      # k-fold CV error
      mclust.cv.err[i] <- Adult.mclust.summary$err.newdata
    }
    return(mean(mclust.cv.err))
  }
  else if (model=='mclust2'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Mclust with EDDA
      Adult.mclust2 <- MclustDA(train[, predictors], train$income, modelType='EDDA')
      # Model summary
      Adult.mclust2.summary <- summary(Adult.mclust2, newdata=test[, predictors], newclass=test$income)
      # k-fold CV error
      mclust2.cv.err[i] <- Adult.mclust2.summary$err.newdata
    }
    return(mean(mclust2.cv.err))
  }
  else if (model=='tree'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Classification tree
      Adult.tree <- tree(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)
      # Predictions of income
      Adult.tree.pred <- predict(Adult.tree, newdata=test, type='class')
      # k-fold CV error
      tree.cv.err[i] <- classification.function(Adult.tree.pred, test$income)[1]
    }
    return(mean(tree.cv.err))
  }
  else if (model=='bagging'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Bagging
      Adult.bag <- randomForest(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, 
                                data=train, mtry=5, importance=TRUE)
      # Predictions of income
      Adult.bag.pred <- predict(Adult.bag, newdata=test)
      # k-fold CV error
      bag.cv.err[i] <- classification.function(Adult.bag.pred, test$income)[1]
    }
    return(mean(bag.cv.err))
  }
  else if (model=='randomforest'){
    for (i in 1:k){
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Bagging
      Adult.rf <- randomForest(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, 
                                data=train, importance=TRUE)
      # Predictions of income
      Adult.rf.pred <- predict(Adult.rf, newdata=test)
      # k-fold CV error
      rf.cv.err[i] <- classification.function(Adult.rf.pred, test$income)[1]
    }
    return(mean(rf.cv.err))
  }
  else if (model=='boosting'){
    for (i in 1:k){
      # Convert income to 0-1 outcomes (required for gbm classification)
      data$class <- ifelse(data$income == ' <=50K', 0, 1)
      # Training set
      train <- data[folds != i, ]
      # Validation set
      test <- data[folds == i, ]
      # Boosting
      Adult.boost <- gbm(class ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train,
                         distribution='bernoulli', n.trees=500, interaction.depth=4)
      # Predictions probabilities of income
      Adult.boost.probs <- predict(Adult.boost, newdata=test, n.trees=500, type='response')
      # If predicted probability > 0.5, label it as 1; otherwise, label it as 0
      Adult.boost.pred <- rep(0, length(Adult.boost.probs))
      Adult.boost.pred[Adult.boost.probs > 0.5] <- 1
      # k-fold CV error
      boost.cv.err[i] <- classification.function(Adult.boost.pred, test$class)[1]
    }
    return(mean(boost.cv.err))
  }
}
```

```{r}
# Validation set approach (VSA)
# Logistic regression model using training set
Adult.glm <- glm(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train, family=binomial)

# Predicted probabilities of income
Adult.glm.probs <- predict(Adult.glm, newdata=test, type='response')

# If predicted probability > 0.5, label it as ">50K"; otherwise, label it as "<=50K"
Adult.glm.pred <- rep(' <=50K', length(Adult.glm.probs))
Adult.glm.pred[Adult.glm.probs > 0.5] <- ' >50K'

# Validation set error rate
logistic.val.err <- as.vector(classification.function(Adult.glm.pred, test$income)[1])
# cat('VSA Test Error:', logistic.val.err)
```

```{r}
# 5-fold CV
set.seed(1)
# Logistic regression model using full data set
Adult.glm2 <- glm(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=Adult, family=binomial)

# 5-fold CV error
logistic.cv.err <- cv.glm(Adult, Adult.glm2, K=5)$delta[1]
# cat('5-Fold CV Test Error:', logistic.cv.err)
```

```{r}
# 10-fold CV error
set.seed(1)
logistic.cv.err2 <- cv.glm(Adult, Adult.glm2, K=10)$delta[1]
# cat('10-Fold CV Test Error:', logistic.cv.err2)
```

```{r}
# Validation set approach (VSA)
# LDA model using training set
Adult.lda <- lda(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)

# Predictions of income
Adult.lda.pred <- predict(Adult.lda, newdata=test, type='response')

# Validation set error rate
lda.val.err <- as.vector(classification.function(Adult.lda.pred$class, test$income)[1])
# cat('VSA Test Error:', lda.val.err)
```

```{r}
# 5-fold CV
lda.cv.err <- cv.function(data=Adult, k=5, model='lda')
# cat('5-Fold CV Test Error:', lda.cv.err)
```

```{r}
# 10-fold CV
lda.cv.err2 <- cv.function(data=Adult, k=10, model='lda')
# cat('10-Fold CV Test Error:', lda.cv.err2)
```

```{r}
# Validation set approach (VSA)
# QDA model using training set
Adult.qda <- qda(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)

# Predictions of income
Adult.qda.pred <- predict(Adult.qda, newdata=test, type='response')

# Validation set error rate
qda.val.err <- as.vector(classification.function(Adult.qda.pred$class, test$income)[1])
# cat('VSA Test Error:', qda.val.err)
```

```{r}
# 5-fold CV
qda.cv.err <- cv.function(data=Adult, k=5, model='qda')
# cat('5-Fold CV Test Error:', qda.cv.err)
```

```{r}
# 10-fold CV
qda.cv.err2 <- cv.function(data=Adult, k=10, model='qda')
# cat('10-Fold CV Test Error:', qda.cv.err2)
```

```{r}
# Validation set approach (VSA)
# KNN (K = sqrt(n))
set.seed(1)
train.X <- cbind(train$age, train$education_num, train$capital_gain, train$capital_loss, train$hours_per_week)
test.X <- cbind(test$age, test$education_num, test$capital_gain, test$capital_loss, test$hours_per_week)

# Predictions of income
Adult.knn.pred <- knn(train.X, test.X, train$income, k=sqrt(nrow(train)))

# Validation set error rate
knn.val.err <- as.vector(classification.function(Adult.knn.pred, test$income)[1])
# cat('VSA Test Error:', knn.val.err)
```

```{r}
# 5-fold CV
# knn.cv.err <- cv.function(data=Adult, k=5, model='knn')
knn.cv.err <- 0.1698336  # Save result to save time
# cat('5-Fold CV Test Error:', knn.cv.err)
```

```{r}
# 10-fold CV
# knn.cv.err2 <- cv.function(data=Adult, k=10, model='knn')
knn.cv.err2 <- 0.1682774  # Save result to save time
# cat('10-Fold CV Test Error:', knn.cv.err2)
```

```{r}
# Validation set approach (VSA)
# Mclust
set.seed(1)
predictors <- c('age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week')
Adult.mclust <- MclustDA(train[, predictors], train$income)

# Model summary
Adult.mclust.summary <- summary(Adult.mclust, newdata=test[, predictors], newclass=test$income)

# Validation set error rate
mclust.val.err <- Adult.mclust.summary$err.newdata
# cat('VSA Test Error:', mclust.val.err)
```

```{r}
# 5-fold CV
# mclust.cv.err <- cv.function(data=Adult, k=5, model='mclust')
mclust.cv.err <- 0.2525689  # Save the result to save time
# cat('5-Fold CV Test Error:', mclust.cv.err)
```

```{r}
# 10-fold CV
# mclust.cv.err2 <- cv.function(data=Adult, k=10, model='mclust')
mclust.cv.err2 <- 0.2504199 # Save the result to save time
# cat('10-Fold CV Test Error:', mclust.cv.err2)
```

```{r}
# Validation set approach (VSA)
# Mclust with EDDA
set.seed(1)
Adult.mclust2 <- MclustDA(train[, predictors], train$income, modelType='EDDA')

# Model summary
Adult.mclust2.summary <- summary(Adult.mclust2, newdata=test[, predictors], newclass=test$income)

# Validation set error rate
mclust2.val.err <- Adult.mclust2.summary$err.newdata
# cat('VSA Test Error:', mclust2.val.err)
```

```{r}
# 5-fold CV
mclust2.cv.err <- cv.function(data=Adult, k=5, model='mclust2')
# cat('5-Fold CV Test Error:', mclust2.cv.err)
```

```{r}
# 10-fold CV
mclust2.cv.err2 <- cv.function(data=Adult, k=10, model='mclust2')
# cat('10-Fold CV Test Error:', mclust2.cv.err2)
```

## Summary of Previous Classification Methods

```{r}
# Vector of model names
model.vector <- c('Logistic Regression', 'LDA', 'QDA', 'KNN (K=sqrt(n))', 'MclustDA', 
                  'MclustDA with EDDA')

# Vectors of test error rates 
logistic.err <- c(logistic.val.err, logistic.cv.err, logistic.cv.err2)
lda.err <- c(lda.val.err, lda.cv.err, lda.cv.err2)
qda.err <- c(qda.val.err, qda.cv.err, qda.cv.err2)
knn.err <- c(knn.val.err, knn.cv.err, knn.cv.err2)
mclust.err <- c(mclust.val.err, mclust.cv.err, mclust.cv.err2)
mclust2.err <- c(mclust2.val.err, mclust2.cv.err, mclust2.cv.err2)
# Matrix of test error rates
err.matrix <- round(rbind(logistic.err, lda.err, qda.err, knn.err, 
                          mclust.err, mclust2.err), 4)
rownames(err.matrix) <- NULL
colnames(err.matrix) <- c('VSA', '5-Fold CV', '10-Fold CV')

dt <- cbind(Method=model.vector, err.matrix)
kable(dt, align='l', caption='Test Error Rate Comparison for the Classification Methods')
```

**Table 1** shows the VSA, 5-fold, and 10-fold CV test errors associated with each model. Note that I omitted the **rpart** classification tree since we will be using the **tree** package later.

5-fold or 10-fold CV are more reliable than VSA since their test error rate estimates suffer neither from high bias nor high variance, while the estimate using VSA is highly variable. Logistic regression performed the best since it has the lowest 5-fold and 10-fold CV test errors, although KNN did have the best VSA test error. On the other hand, MclustDA by far performed the worst for all three cross-validation methods.

## Tree-based Classification Methods
We now use tree-based classification methods to model the data.

### Classification Tree
We fit a classification tree to the training set using **tree()** from the **tree** package.  

```{r}
# Validation set approach (VSA)
# Classification tree
Adult.tree <- tree(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train)

# Predictions of income
Adult.tree.pred <- predict(Adult.tree, newdata=test, type='class')

# Validation set error rate
tree.val.err <- as.vector(classification.function(Adult.tree.pred, test$income)[1])
cat('VSA Test Error:', tree.val.err)
```

```{r}
# 5-fold CV
tree.cv.err <- cv.function(data=Adult, k=5, model='tree')
cat('5-Fold CV Test Error:', tree.cv.err)
```

```{r}
# 10-fold CV
tree.cv.err2 <- cv.function(data=Adult, k=10, model='tree')
cat('10-Fold CV Test Error:', tree.cv.err2)
```

### Bagging
We use **randomForest()** from the **randomForest** package to perform bagging. Bagging is simply random forest with *m = p*, so **randomForest()** can be used to perform both. The argument mtry=5 indicates that all 5 predictors should be considered for each split of the tree. By default, 500 trees are grown.

```{r}
# Validation set approach (VSA)
# Bagging
set.seed(1)
Adult.bag <- randomForest(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train, 
                          mtry=5, importance=TRUE)

# Predictions of income
Adult.bag.pred <- predict(Adult.bag, newdata=test)

# Validation set error rate
bag.val.err <- as.vector(classification.function(Adult.bag.pred, test$income)[1])
cat('VSA Test Error:', bag.val.err)
```

```{r}
# 5-fold CV
# bag.cv.err <- cv.function(data=Adult, k=5, model='bagging')
bag.cv.err <- 0.1742149  # Save the result to save time
cat('5-Fold CV Test Error:', bag.cv.err)
```

```{r}
# 10-fold CV
# bag.cv.err2 <- cv.function(data=Adult, k=10, model='bagging')
bag.cv.err2 <- 0.173007  # Save the result to save time
cat('10-Fold CV Test Error:', bag.cv.err2)
```

### Random Forests
Similar to bagging, We use **randomForest()** to perform random forests. In random forests, only a random subset of the predictors is considered for each split; by default, **randomForest()** uses $m \approx \sqrt{p}$ when building a random forest of classification trees. In this case, *m* = 2 since there are 5 predictors.

```{r}
# Validation set approach (VSA)
# Random forests
set.seed(1)
Adult.rf <- randomForest(income ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train,
                         importance=TRUE)

# Predictions of income
Adult.rf.pred <- predict(Adult.rf, newdata=test)

# Validation set error rate
rf.val.err <- as.vector(classification.function(Adult.rf.pred, test$income)[1])
cat('VSA Test Error:', rf.val.err)
```

```{r}
# 5-fold CV
# rf.cv.err <- cv.function(data=Adult, k=5, model='randomforest')
rf.cv.err <- 0.1609682  # Save the result to save time
cat('5-Fold CV Test Error:', rf.cv.err)
```

```{r}
# 10-fold CV
# rf.cv.err2 <- cv.function(data=Adult, k=10, model='randomforest')
rf.cv.err2 <- 0.1600673  # Save the result to save time
cat('10-Fold CV Test Error:', rf.cv.err2)
```

### Boosting
We use **gbm()** from the **gbm** package to perform boosting on the training set with 500 trees and the default shrinkage value: 0.1. First, the response (*income*) must be converted to 0-1 outcomes in order to fit boosted classification trees. In this case, incomes of <=50K are converted to 0's, and incomes of >50K are converted to 1's.

```{r}
# Validation set approach (VSA)
# Convert income to 0-1 outcomes (required for gbm classification)
train$class <- ifelse(train$income == ' <=50K', 0, 1)
test$class <- ifelse(test$income == ' <=50K', 0, 1)
# Boosting
set.seed(1)
Adult.boost <- gbm(class ~ age + education_num + capital_gain + capital_loss + hours_per_week, data=train, 
                   distribution='bernoulli', n.trees=500, interaction.depth=4)

# Predictions probabilities of income
Adult.boost.probs <- predict(Adult.boost, newdata=test, n.trees=500, type='response')

# If predicted probability > 0.5, label it as 1; otherwise, label it as 0
Adult.boost.pred <- rep(0, length(Adult.boost.probs))
Adult.boost.pred[Adult.boost.probs > 0.5] <- 1

# Validation set error rate
boost.val.err <- as.vector(classification.function(Adult.boost.pred, test$class)[1])
cat('VSA Test Error:', boost.val.err)
```

```{r}
# 5-fold CV
# boost.cv.err <- cv.function(data=Adult, k=5, model='boosting')
boost.cv.err <- 0.1569348  # Save result to save time
cat('5-Fold CV Test Error:', boost.cv.err)
```

```{r}
# 10-fold CV
# boost.cv.err2 <- cv.function(data=Adult, k=10, model='boosting')
boost.cv.err2 <- 0.1563819  # Save result to save time
cat('10-Fold CV Test Error:', boost.cv.err2)
```

\newpage

## Summary of Classification Methods Including Tree-Based Methods

```{r}
# Vector of model names
model.vector <- c('Logistic Regression', 'LDA', 'QDA', 'KNN (K=sqrt(n))', 'MclustDA', 
                  'MclustDA with EDDA', 'Classification Tree', 'Bagging', 'Random Forests',
                  'Boosting')

# Vectors of test error rates 
logistic.err <- c(logistic.val.err, logistic.cv.err, logistic.cv.err2)
lda.err <- c(lda.val.err, lda.cv.err, lda.cv.err2)
qda.err <- c(qda.val.err, qda.cv.err, qda.cv.err2)
knn.err <- c(knn.val.err, knn.cv.err, knn.cv.err2)
mclust.err <- c(mclust.val.err, mclust.cv.err, mclust.cv.err2)
mclust2.err <- c(mclust2.val.err, mclust2.cv.err, mclust2.cv.err2)
tree.err <- c(tree.val.err, tree.cv.err, tree.cv.err2)
bag.err <- c(bag.val.err, bag.cv.err, bag.cv.err2)
rf.err <- c(rf.val.err, rf.cv.err, rf.cv.err2)
boost.err <- c(boost.val.err, boost.cv.err, boost.cv.err2)

# Matrix of test error rates
err.matrix <- round(rbind(logistic.err, lda.err, qda.err, knn.err, 
                          mclust.err, mclust2.err, tree.err, bag.err,
                          rf.err, boost.err), 4)
rownames(err.matrix) <- NULL
colnames(err.matrix) <- c('VSA', '5-Fold CV', '10-Fold CV')

dt <- cbind(Method=model.vector, err.matrix)
kable(dt, align='l', caption='Test Error Rate Comparison for the Classification Methods')
```

**Table 2** shows the VSA, 5-fold, and 10-fold CV test errors associated with each model including the tree-based methods.

We see that bagging, random forests, and boosting sequentially improve upon the single classification tree; however, logistic regression is still the best in terms of 5-fold and 10-fold CV test errors. 