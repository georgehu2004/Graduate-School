---
title: "STAT 602 Homework 12"
author: "Yuchi Hu"
date: "April 12, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=F,warning=F,echo=F,fig.height=3,fig.width=6,cache=F)
```

```{r, eval=F}
# Install packages used in this homework
install.packages(ggdendro)
install.packages(ggplot2)
install.packages(knitr)
```

```{r}
# Load packages used in this homework
library(ggdendro)
library(ggplot2)
library(knitr)
```

# 1. Question 10.7.8 pg 416
In Section 10.2.3, a formula for calculating PVE was given in Equation 10.8. We also saw that the PVE can be obtained using the **sdev** output of the **prcomp()** function.

On the **USArrests** data, calculate PVE in two ways:

## (a) 
Using the **sdev** output of the **prcomp()** function, as was done in Section 10.2.3.

## Answer
We perform principal components analysis using **prcomp()**. The variance explained by each principal component is obtained by squaring the standard deviation of each principal component (**sdev** output of **prcomp()**). The proportion of variance explained (PVE) by each principal component is computed by dividing the variance explained by each principal component by the total variance explained by all four principal components:

```{r}
# Load the USArrests data
data(USArrests)

# Perform PCA
pr.out <- prcomp(USArrests, scale=TRUE)
# Variance explained by each principal component
pr.var <- pr.out$sdev^2
# PVE by each principal component
pve <- pr.var/sum(pr.var)
pve
```

We see that the first principal component explains 62% of the variance in the data, the second principal component explains 24.7% of the variance, and so forth.

## (b)
By applying Equation 10.8 directly. That is, use the **prcomp()** function to compute the principal component loadings. Then, use those loadings in Equation 10.8 to obtain the PVE.

These two approaches should give the same results.

## Answer
Equation 10.8 gives the PVE of the *m*th principal component:

$$\frac{\sum_{i=1}^n (\sum_{j=1}^p \phi_{jm}x_{ij})^2}{\sum_{j=1}^p \sum_{i=1}^n x_{ij}^2}$$

where $\phi_{jm}$ is the loading for the *j*th variable on the *m*th principal component and $x_{ij}$ is the value of the *j*th variable for the *i*th observation.

\newpage

In part (a), we performed PCA using standardized variables with mean zero and standard deviation one, so first we use **scale()** to standardize the variables. Then, we use **prcomp()** to compute the principal component loadings. Using equation 10.8 with the standardized variables and the loadings, we calculate PVE:

```{r}
# Principal component loadings
loadings <- pr.out$rotation
# Standardize the variables
USArrests.scaled <- scale(USArrests)
# Numerator of equation 10.8
num <- apply((USArrests.scaled %*% loadings)^2, 2, sum)
# Denominator of equation 10.8
denom <- sum(USArrests.scaled^2)
# PVE by each principal component
pve <- num/denom
pve
```

We see that the results are equal to those from part (a).

# 2. Question 10.7.9 pg 416
Consider the **USArrests** data. We will now perform hierarchical clustering on the states.

## (a)
Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

## Answer
We use hierarchical clustering with complete linkage and Euclidean distance to cluster the states. **hclust()** implements hierarchical clustering, and **plot()** plots the resulting dendrogram. To plot the dendrogram in ggplot2, we use the **ggdendro** library. The dendrogram is shown below:

```{r, fig.height=7, fig.width=11}
# Dendrogram using complete linkage
hc.complete <- hclust(dist(USArrests), method='complete')
plot(hc.complete, main='USArrests Dendrogram', xlab='', sub='', cex=0.9)
```

```{r, fig.height=5, fig.width=7}
# ggplot2
theme_update(plot.title=element_text(hjust=0.5))
ggdendrogram(hc.complete, rotate=FALSE) + labs(title='GGPLOT2: USArrests Dendrogram')
```

\newpage

## (b)
Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

## Answer
We use **cutree()** to cut the dendrogram at a height that results in three distinct clusters. The table below shows which states belong to which clusters.

```{r}
# Cut the dendrogram into three clusters
clusters <- cutree(hc.complete, 3)
# States in each cluster
cluster1 <- rownames(USArrests[which(clusters==1),])
cluster2 <- rownames(USArrests[which(clusters==2),])
cluster3 <- rownames(USArrests[which(clusters==3),])

# Table of states in each cluster
dt <- cbind('Cluster 1'=c(cluster1, rep('', 4)), 'Cluster 2'=c(cluster2, rep('', 6)), 'Cluster 3'=cluster3)
kable(dt, align='c', caption='States in Each Cluster')
```

## (c)
Hierarchically cluster the states using complete linkage and Euclidean distance, *after scaling the variables to have standard deviation one*.

## Answer
We use **scale()** to scale the variables to have standard deviation one, then hierarchically cluster the states using complete linkage and Euclidean distance. The dendrogram is shown below:

```{r, fig.height=7, fig.width=11}
# Scale the variables
USArrests.scaled <- scale(USArrests)

# Dendrogram using complete linkage
hc.complete.scaled <- hclust(dist(USArrests.scaled), method='complete')
plot(hc.complete.scaled, main='USArrests Dendrogram (Scaled Variables)', xlab='', sub='', cex=0.9)
```

```{r, fig.height=5, fig.width=7}
# ggplot2
ggdendrogram(hc.complete.scaled, rotate=FALSE) + labs(title='GGPLOT2: USArrests Dendrogram (Scaled Variables)')
```

## (d)
What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

## Answer
The table below shows which states belong to which clusters after scaling the variables.

```{r}
# Cut the dendrogram into three clusters
clusters <- cutree(hc.complete.scaled, 3)
# States in each cluster
cluster1 <- rownames(USArrests[which(clusters==1),])
cluster2 <- rownames(USArrests[which(clusters==2),])
cluster3 <- rownames(USArrests[which(clusters==3),])

# Table of states in each cluster
dt <- cbind('Cluster 1'=c(cluster1, rep('', 23)), 'Cluster 2'=c(cluster2, rep('', 20)), 'Cluster 3'=cluster3)
kable(dt, align='c', caption='States in Each Cluster (Scaled Variables)')
```

Without scaling the variables, the three clusters are somewhat balanced; however, we see that after scaling the variables, cluster 3 has many more states in it than cluster 1 and cluster 2. Thus, we obtain very different clusters after scaling the variables.

The variables should be scaled before the inter-observation dissimilarities are computed since the variables are measured in different units. *Murder*, *Assault*, and *Rape* are rates calculated per 100,000 people, while *UrbanPop* is the percentage of the population living in urban areas. If we do not scale the variables, the first principle component loading vector would place most of its weight on *Assault* since that variables has by far the highest variance.

# 3. Question 10.7.11 pg 417
On the book website, www.StatLearning.com, there is a gene expression data set (**Ch10Ex11.csv**) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.

## (a)
Load in the data using **read.csv()**. You will need to select **header=F**.

## Answer
We load in the data using **read.csv()**.

```{r}
# Load Ch10Ex11.csv
path <- 'C:/Users/George/Desktop/ModernAppliedStatisticsII/Ch10Ex11.csv'
gene_exp <- read.csv(path, header=F)
```

## (b)
Apply hierarchical clustering to the samples using correlation-based distance, and plot the dendrogram. Do the genes separate the samples into the two groups? Do your results depend on the type of linkage used?

## Answer
We apply hierarchical clustering to the samples using correlation-based distance, i.e. 1 - Abs(Correlation). We plot the resulting dendrograms with different types of linkage: complete, average, and single.

### Complete Linkage

```{r}
dd <- as.dist(1-abs(cor(gene_exp)))
# Dendrogram using complete linkage
hc.complete <- hclust(dd, method='complete')
plot(hc.complete, main='Gene Expression Dendrogram (Complete Linkage)', xlab='', sub='', cex=0.9)

# ggplot2
ggdendrogram(hc.complete, rotate=FALSE) + labs(title='GGPLOT2: Gene Expression Dendrogram (Complete Linkage)')
```

We can use **cutree()** to cut the dendrogram at a height that results in two distinct clusters. The cluster labels for each sample are:

```{r}
# Cut the dendrogram into two clusters
cutree(hc.complete, 2)
```

The first 20 samples (V1-V20) are from healthy patients, while the second 20 (V21-V40) are from a diseased group. With complete linkage, we see that all of the healthy samples are in one cluster, while all of the diseased samples are in another cluster. Hence, we have perfect separation.

### Average Linkage

```{r}
# Dendrogram using average linkage
hc.average <- hclust(dd, method='average')
plot(hc.average, main='Gene Expression Dendrogram (Average Linkage)', xlab='', sub='', cex=0.9)

# ggplot2
ggdendrogram(hc.average, rotate=FALSE) + labs(title='GGPLOT2: Gene Expression Dendrogram (Average Linkage)')
```

The cluster labels for each sample are:

```{r}
# Cut the dendrogram into two clusters
cutree(hc.average, 2)
```

With average linkage, we see that V14 and V16 belong to their own cluster, while all of the other samples are in another cluster. 

### Single Linkage

```{r}
# Dendrogram using single linkage
hc.single <- hclust(dd, method='single')
plot(hc.single, main='Gene Expression Dendrogram (Single Linkage)', xlab='', sub='', cex=0.9)

# ggplot2
ggdendrogram(hc.single, rotate=FALSE) + labs(title='GGPLOT2: Gene Expression Dendrogram (Single Linkage)')
```

The cluster labels for each sample are:

```{r}
# Cut the dendrogram into two clusters
cutree(hc.single, 2)
```

With single linkage, we see that V12 and V19 belong to their own cluster, while all of the other samples are in another cluster.

Thus, the results certainly do depend on the type of linkage. Complete linkage perfectly separates the healthy and diseased samples into their correct clusters, while average and single linkage found two samples that belong to their own cluster.

## (c)
Your collaborator wants to know which genes differ the most across the two groups. Suggest a way to answer this question, and apply it here.

## Answer
Since the first 20 samples are healthy and the second 20 are diseased, we can create labels for each sample. Then, we project the samples onto the first two principal components (plot the scores for the first two principal components) to see if there is good separation of the samples.

```{r}
# Create labels for each sample (1=healthy, 2=diseased)
Group <- factor(rep(c('Healthy', 'Diseased'), each=20), levels=c('Healthy', 'Diseased'))

# Perform PCA
pr.out <- prcomp(t(gene_exp), scale=TRUE)
```

```{r, fig.height=4, fig.width=6}
# First two principal component scores
palette(c('#E69F00', '#56B4E9'))
plot(pr.out$x[,1:2], col=Group, pch=19, cex.main=1, 
     main='First Two Principal Component Scores \nfor the Gene Expression Data', xlab='Z1', ylab='Z2')
legend('top', legend=levels(Group), col=1:2, pch=19, cex=0.8, title='Group')

# ggplot2
ggplot(data=NULL, aes(x=pr.out$x[,1], y=pr.out$x[,2], color=Group)) + geom_point() + 
  labs(title='GGPLOT2: First Two Principal Component Scores \nfor the Gene Expression Data', x='Z1',  y='Z2') +
  scale_color_manual(values=c('#E69F00', '#56B4E9'))
```

We see that there is perfect separation of the samples on the first principal component. 

Next, we calculate the loading for each gene on the first principal component and then sort the absolute values of these loadings. The larger the loading for a gene on the first principal component, the higher its variance is among the samples. The genes with the 10 largest loadings are:

```{r}
# Genes with the 10 largest loadings on the first principal component
order(abs(pr.out$rotation[,1]), decreasing=TRUE)[1:10]
```

Thus, these are the 10 genes that differ the most across the two groups.